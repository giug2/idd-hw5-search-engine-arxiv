<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Whisper-UT: A Unified Translation Framework for Speech and Text</title>
<!--Generated on Fri Sep 19 19:33:14 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2509.16375v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S1" title="In Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S2" title="In Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S2.SS1" title="In 2 Related Work ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Whisper</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S2.SS2" title="In 2 Related Work ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Multi-modal/-task Speech Systems</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S3" title="In Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S3.SS1" title="In 3 Methodology ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Translation with Multi-modal Inputs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S3.SS2" title="In 3 Methodology ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Translation with Speech-only Inputs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S3.SS3" title="In 3 Methodology ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Translation with Text-only Inputs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S3.SS4" title="In 3 Methodology ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Whisper-UT: Unified Translation System</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S3.SS4.SSS1" title="In 3.4 Whisper-UT: Unified Translation System ‣ 3 Methodology ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>3-way Parallel Data Objectives</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S3.SS4.SSS2" title="In 3.4 Whisper-UT: Unified Translation System ‣ 3 Methodology ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Text-Only Data Objectives</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S3.SS4.SSS3" title="In 3.4 Whisper-UT: Unified Translation System ‣ 3 Methodology ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>Dynamic Loss Weighting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S3.SS4.SSS4" title="In 3.4 Whisper-UT: Unified Translation System ‣ 3 Methodology ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.4 </span>Utterance-Level Task Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S3.SS4.SSS5" title="In 3.4 Whisper-UT: Unified Translation System ‣ 3 Methodology ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.5 </span>Error Simulation in Multi-Modal Translation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S3.SS5" title="In 3 Methodology ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Unified Training Framework</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4" title="In Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4.SS1" title="In 4 Experiments ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Tasks and Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4.SS2" title="In 4 Experiments ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4.SS3" title="In 4 Experiments ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4.SS4" title="In 4 Experiments ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4.SS4.SSS1" title="In 4.4 Experimental Results ‣ 4 Experiments ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4.SS4.SSS2" title="In 4.4 Experimental Results ‣ 4 Experiments ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>Cross-task Synergy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4.SS4.SSS3" title="In 4.4 Experimental Results ‣ 4 Experiments ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.3 </span>ASR</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4.SS4.SSS4" title="In 4.4 Experimental Results ‣ 4 Experiments ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.4 </span>MT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4.SS4.SSS5" title="In 4.4 Experimental Results ‣ 4 Experiments ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.5 </span>MMT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4.SS4.SSS6" title="In 4.4 Experimental Results ‣ 4 Experiments ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.6 </span>ST</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4.SS4.SSS7" title="In 4.4 Experimental Results ‣ 4 Experiments ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.7 </span>Summary</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S5" title="In Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S6" title="In Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Limitations and Ethical Considerations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A1" title="In Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Training Detail</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A1.SS1" title="In Appendix A Training Detail ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Parameter Efficient Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A1.SS2" title="In Appendix A Training Detail ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Hyperparameter Settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A1.SS3" title="In Appendix A Training Detail ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Data Augmentation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A2" title="In Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>CTS Data Detail</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A2.SS1" title="In Appendix B CTS Data Detail ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Pre-processing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A2.SS2" title="In Appendix B CTS Data Detail ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>BBN-Mandarin Data Specification</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A3" title="In Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Qualitative Analysis of Code-Switching</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A4" title="In Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Ablation Study</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A4.SS1" title="In Appendix D Ablation Study ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.1 </span>Text-only MT Training and Its Effects</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A4.SS2" title="In Appendix D Ablation Study ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.2 </span>Effectiveness of Multi-task Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A4.SS3" title="In Appendix D Ablation Study ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.3 </span>MMT-Multi-task Training and Its Implications</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A4.SS4" title="In Appendix D Ablation Study ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.4 </span>Unified Translation (UT) Training</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A4.SS4.SSS1" title="In D.4 Unified Translation (UT) Training ‣ Appendix D Ablation Study ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.4.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A4.SS4.SSS2" title="In D.4 Unified Translation (UT) Training ‣ Appendix D Ablation Study ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.4.2 </span>Analysis of Transcript-Conditioning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A4.SS5" title="In Appendix D Ablation Study ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.5 </span>Impact of Out-of-Domain Text Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A4.SS5.SSS1" title="In D.5 Impact of Out-of-Domain Text Data ‣ Appendix D Ablation Study ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.5.1 </span>Dataset Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A4.SS5.SSS2" title="In D.5 Impact of Out-of-Domain Text Data ‣ Appendix D Ablation Study ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.5.2 </span>Analysis of OOD Text Data Injection</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A5" title="In Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Model Training Data Overview</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Whisper-UT: A Unified Translation Framework for Speech and Text</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold">Cihan Xiao<sup class="ltx_sup">1</sup></span>,
<span class="ltx_text ltx_font_bold">Matthew Wiesner<sup class="ltx_sup">1, 2</sup></span>,
<span class="ltx_text ltx_font_bold">Debashish Chakraborty<sup class="ltx_sup">2</sup></span>,
<span class="ltx_text ltx_font_bold">Reno Kriz<sup class="ltx_sup">2</sup></span>,

<br class="ltx_break"/><span class="ltx_text ltx_font_bold">Keith Cunningham<sup class="ltx_sup">3</sup></span>,
<span class="ltx_text ltx_font_bold">Kenton Murray<sup class="ltx_sup">1, 2</sup></span>,
<span class="ltx_text ltx_font_bold">Kevin Duh<sup class="ltx_sup">1, 2</sup></span>,
<span class="ltx_text ltx_font_bold">Luis Tavarez-Arce <sup class="ltx_sup">2</sup></span>,

<br class="ltx_break"/><span class="ltx_text ltx_font_bold">Paul McNamee<sup class="ltx_sup">2</sup></span>,
<span class="ltx_text ltx_font_bold">Sanjeev Khudanpur<sup class="ltx_sup">1, 2</sup></span>
<br class="ltx_break"/>
<br class="ltx_break"/><sup class="ltx_sup">1</sup>Center for Language and Speech Processing, Johns Hopkins University
<br class="ltx_break"/><sup class="ltx_sup">2</sup>Human Language Technology Center of Excellence, Johns Hopkins University
<br class="ltx_break"/><sup class="ltx_sup">3</sup>Georgetown University

<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold">Correspondence:</span> <a class="ltx_ref ltx_href" href="mailto:cxiao7@jhu.edu" title="">cxiao7@jhu.edu</a>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Encoder-decoder models have achieved remarkable success in speech and text tasks, yet efficiently adapting these models to diverse uni/multi-modal scenarios remains an open challenge. In this paper, we propose Whisper-UT, a unified and efficient framework that leverages lightweight adapters to enable seamless adaptation across tasks, including a multi-modal machine translation (MMT) task that explicitly conditions translation on both speech and source language text inputs. By incorporating ASR hypotheses or ground-truth transcripts as prompts, this approach not only enables the system to process both modalities simultaneously but also enhances speech translation (ST) performance through a 2-stage decoding strategy. We demonstrate our methods using the Whisper model, though in principle they are general and could be applied to similar multitask models. We highlight the effectiveness of cross-modal and cross-task fine-tuning, which improves performance without requiring 3-way parallel data. Our results underscore the flexibility, efficiency, and general applicability of the proposed framework for multi-modal translation.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom">
<p class="ltx_p ltx_align_center"><span class="ltx_text ltx_font_bold">Whisper-UT: A Unified Translation Framework for Speech and Text</span></p>
<p class="ltx_p ltx_align_center" style="width:345.0pt;"><span class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span class="ltx_tr">
<span class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">
Cihan Xiao<sup class="ltx_sup">1</sup>,
Matthew Wiesner<sup class="ltx_sup">1, 2</sup>,
Debashish Chakraborty<sup class="ltx_sup">2</sup>,
Reno Kriz<sup class="ltx_sup">2</sup>,</span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">Keith Cunningham<sup class="ltx_sup">3</sup></span>,
<span class="ltx_text ltx_font_bold">Kenton Murray<sup class="ltx_sup">1, 2</sup></span>,
<span class="ltx_text ltx_font_bold">Kevin Duh<sup class="ltx_sup">1, 2</sup></span>,
<span class="ltx_text ltx_font_bold">Luis Tavarez-Arce <sup class="ltx_sup">2</sup></span>,</span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">Paul McNamee<sup class="ltx_sup">2</sup></span>,
<span class="ltx_text ltx_font_bold">Sanjeev Khudanpur<sup class="ltx_sup">1, 2</sup></span></span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_center"><sup class="ltx_sup">1</sup>Center for Language and Speech Processing, Johns Hopkins University</span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_center"><sup class="ltx_sup">2</sup>Human Language Technology Center of Excellence, Johns Hopkins University</span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_center"><sup class="ltx_sup">3</sup>Georgetown University</span></span>
<span class="ltx_tr">
<span class="ltx_td ltx_align_center">
<span class="ltx_text ltx_font_bold">Correspondence:</span> <a class="ltx_ref ltx_href" href="mailto:cxiao7@jhu.edu" title="">cxiao7@jhu.edu</a></span></span>
</span>
</span></span></p>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p">The task of speech-to-text translation (ST) encompasses converting spoken content from one language to another, aiming to overcome language barriers to communication. Traditionally, the task involves an automatic speech recognition (ASR) module to transcribe spoken words, followed by a machine translation (MT) module to convert the transcribed text into the target language in a cascaded manner <cite class="ltx_cite ltx_citemacro_cite">Ney (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib21" title="">1999</a>)</cite>. The recent development of end-to-end neural architectures and large pre-trained models have substantially propelled advancements in downstream speech tasks, via either self-supervised learning (SSL) <cite class="ltx_cite ltx_citemacro_cite">Baevski et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib1" title="">2020</a>); Hsu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib11" title="">2021</a>); Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib4" title="">2022</a>)</cite> or fully supervised learning. Among the pre-trained acoustic models, Whisper <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib27" title="">2022</a>)</cite>, a transformer-based encoder-decoder multi-task model trained with large-scale data in a supervised manner, has exhibited good performance on various ST corpora.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p">However, in real-world scenarios, input modalities and data conditions vary widely. In offline settings, for instance, translating conversational or dialectal speech—characterized by disfluencies, code-switching, and noisy acoustic environments—poses significant challenges to end-to-end models, often resulting in degraded performance. Conversely, scenarios like business meetings or translated media archives frequently provide both source-language speech and (manual or ASR-generated) transcripts. Yet existing systems fail to exploit this multi-modal synergy.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p">To address this, we systematically investigate how multi-task encoder-decoder models—using Whisper as a representative case study—can be efficiently adapted to these heterogeneous scenarios. First, we examine fine-tuning strategies for conventional ST (using 3-way parallel speech-transcript-translation data), speech-to-text tasks (ASR-only data), and MT, while also methods for multi-modal translation where both speech and transcripts are available. Our analysis reveals two key insights:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Cross-task training induces synergistic benefits</span>—fine-tuning on in-domain ASR data improves ST performance, while ST training conversely enhances ASR accuracy, suggesting mutual reinforcement between the ASR and ST tasks even without 3-way parallel data;</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Multi-modal inputs (speech + text) consistently enhance translation quality when fused</span>, even with imperfect ASR transcripts.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p">Building on these findings, we propose <span class="ltx_text ltx_font_bold">Whisper</span> for <span class="ltx_text ltx_font_bold">U</span>nified <span class="ltx_text ltx_font_bold">T</span>ranslation<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We open source our code at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/BorrisonXiao/Whisper-UT" title="">https://github.com/BorrisonXiao/Whisper-UT</a>.</span></span></span>, or <span class="ltx_text ltx_font_bold">Whisper-UT</span>, a framework that transforms Whisper’s decoder into a unified conditional generation model, capable of dynamically conditioning on speech, text, or both modalities. The framework repurposes Whisper’s encoder-decoder architecture as a versatile multi-modal interface through two innovations:</p>
<ul class="ltx_itemize" id="S1.I2">
<li class="ltx_item" id="S1.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1</span>
<div class="ltx_para" id="S1.I2.ix1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">A multi-task learning paradigm</span> with a stochastic task-selection mechanism to adapt the system across ASR, MT, ST, and multimodal translation tasks using a single set of LoRA parameters;</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2</span>
<div class="ltx_para" id="S1.I2.ix2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">A two-stage decoding strategy</span>, where the decoder first generates an ASR transcript from speech, then reuses it as context for translation, perhaps emulating human thought processes, even when a transcript is not provided.</p>
</div>
</li>
</ul>
<p class="ltx_p">Crucially, Whisper-UT requires no architectural modifications—only fine-tuning—ensuring compatibility with any encoder-decoder model.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p">Experiments on CoVoST2’s <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib36" title="">2020b</a>)</cite> French-English (<span class="ltx_text ltx_font_typewriter">fr-en</span>) and German-English (<span class="ltx_text ltx_font_typewriter">de-en</span>) subsets demonstrate strong performance. Extended evaluations on conversational telephone speech (CTS) corpora—Fisher-CallHome Spanish <cite class="ltx_cite ltx_citemacro_cite">Post et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib26" title="">2013</a>)</cite>, and BBN Mandarin-English <cite class="ltx_cite ltx_citemacro_cite">Wotherspoon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib39" title="">2024</a>)</cite> further confirm the robustness of our approach across diverse domains. Notably, Whisper-UT outperforms the 1.3B-parameter NLLB model in multi-modal settings (speech + ground-truth text) and achieves superior speech-only translation via hypothesis prompting.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p">Our work highlights the untapped potential of multi-task models in adaptive translation systems. By unifying modality handling and enabling efficient task specialization, Whisper-UT bridges the gap between rigid single-modality systems and the dynamic needs of real-world applications.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Whisper</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p">Whisper is an end-to-end multi-task speech model that adopts a transformer-like encoder-decoder architecture. Its <span class="ltx_text ltx_font_smallcaps">large-v2</span> version is pre-trained on 680,000 hours of speech data with multiple supervision.
As with the original transformer model <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib34" title="">2023</a>)</cite>, the loss function Whisper used at its pre-training time is the cross-entropy objective for all tasks.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p">Whisper’s decoder supports a prompting mechanism, originally designed for better capturing long-range dependencies of the transcripts/translations to resolve local audio ambiguities. Particularly, long utterances are segmented into chunks and the decoder generates its hypothesis for the current segment conditioning on the previous segment’s transcripts. Inspired by the effectiveness of GPT-like decoder-only models in machine translation, we hypothesize that Whisper’s decoder, which may be viewed as an audio-conditional language model, is also capable of performing audio-augmented text generation conditioning on <span class="ltx_text ltx_font_italic">both inputs</span>. Our work extends recent work showing that the Whisper can be adapted via fine-tuning to perform a number of novel tasks including, audio-visual speech recognition <cite class="ltx_cite ltx_citemacro_cite">Rouditchenko et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib29" title="">2024</a>)</cite>, target-speaker ASR <cite class="ltx_cite ltx_citemacro_cite">Guo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib10" title="">2024</a>); Polok et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib25" title="">2024</a>); Ma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib19" title="">2024a</a>)</cite>, translation to non-English languages <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib23" title="">2023</a>)</cite>, by showing that Whisper can be extended to enable multi-modal translation, i.e., using either only text or both text and speech inputs simultaneously.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Multi-modal/-task Speech Systems</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p">Recent developments in multi-modal and multi-task systems, e.g., <cite class="ltx_cite ltx_citemacro_cite">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib32" title="">2021</a>)</cite>, are exploring new ways to combine audio and text to improve various language-related tasks. mSLAM <cite class="ltx_cite ltx_citemacro_cite">Bapna et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib2" title="">2022</a>)</cite>, a multilingual speech and language model, has emerged as a pioneering approach. It aims to construct a shared representation space for both speech and text through joint pre-training on both self-supervised and supervised tasks with various loss objectives, including translation language modeling (TLM) loss for ST.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p">SeamlessM4T <cite class="ltx_cite ltx_citemacro_cite">Communication et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib8" title="">2023</a>)</cite> is another innovative model that further refines the integration of multi-modal inputs for speech and text translation tasks. As a single model designed for ASR, T2T translation, T2S translation, S2T translation and S2S translation, it consists of multiple building blocks to leverage uni-modal data, including a w2v-BERT <cite class="ltx_cite ltx_citemacro_cite">Chung et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib7" title="">2021</a>)</cite> as the speech encoder, a 1.3B NLLB model <cite class="ltx_cite ltx_citemacro_cite">Team et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib33" title="">2022</a>)</cite> as the text encoder and decoder, a transformer-based text-to-unit encoder-decoder model for speech, with a vocoder for converting the unit-sequences to waveforms.
These systems, along with most existing methods, primarily seek to simply align the representations of the text and speech modalities, limiting the model to still accept only one input modality at a time during inference, which prevents exploitation of <span class="ltx_text ltx_font_italic">cross-modal cues</span>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p">More recently, speech‑centric large language models such as QWen‑Audio <cite class="ltx_cite ltx_citemacro_cite">Chu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib6" title="">2024</a>)</cite> have shown that a unified decoder can be fine‑tuned for a broad spectrum of text‑conditioned speech tasks—including contextual ASR <cite class="ltx_cite ltx_citemacro_cite">Xiao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib40" title="">2025</a>)</cite>—but these approaches rely on massive pretrained text LLMs and demand extensive data and compute during fine‑tuning. This is a gap we aim to fill.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p">A number of related works <cite class="ltx_cite ltx_citemacro_cite">Ma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib20" title="">2024b</a>); Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib42" title="">2023</a>); Liu and Niehues (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib18" title="">2024</a>); Le et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib17" title="">2024</a>)</cite> have also demonstrated that multi-task learning can greatly improve speech translation performance.
Here, we focus on model fine-tuning and demonstrate that training end-to-end models for either ASR or ST alone improves performance on the other task, enabling fine-tuning with data that was not original annotated for the target domain task.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="446" id="S2.F1.g1" src="fig1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold">Overview of our approach.</span> <span class="ltx_text ltx_font_italic">ASR-HYP</span> refers to the ASR hypothesis generated. When GT is used, the task is MMT, otherwise it is referred as 2-Stage-ST. The <math alttext="\oplus" class="ltx_Math" display="inline" id="S2.F1.m2" intent=":literal"><semantics><mo>⊕</mo><annotation encoding="application/x-tex">\oplus</annotation></semantics></math> symbol refers to the XOR operation. Note that special tokens are omitted to simplify illustration. </figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p">Traditional translation systems treat ST, MT, and ASR as distinct tasks, each requiring separate models or specialized architectures. In this work, we propose a <span class="ltx_text ltx_font_bold">unified translation</span> framework that unifies these tasks under a single encoder-decoder paradigm, treating all forms of language conversion—including audio-to-text, text-to-text, and multi-modal translation—as conditional generation tasks. Our approach enables seamless adaptation to various input modalities and data conditions without requiring fundamental architectural changes.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p">At the core of our method is the insight that ASR can be reformulated as a source-language transcription task, ST as a direct speech-to-text translation task, and MT as a standard text-to-text translation task—all of which can be expressed as instances of sequence-to-sequence learning. Extending this idea, we introduce a <span class="ltx_text ltx_font_bold">multi-modal translation</span> task, for which the model conditions on both speech and its corresponding transcript (either human-annotated or ASR-generated) to improve translation quality. This formulation generalizes the conventional ST and MT paradigms, leveraging available transcripts to enhance translation in scenarios where speech alone may be ambiguous or error-prone.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Translation with Multi-modal Inputs</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p">We first provide a formal definition of the multi-modal translation
(MMT) task, or more precisely, the task of speech-and-text-conditioned translation. Let <math alttext="X=(x_{1},x_{2},\cdots,x_{T})" class="ltx_Math" display="inline" id="S3.SS1.p1.m1" intent=":literal"><semantics><mrow><mi>X</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msub><mi>x</mi><mi>T</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">X=(x_{1},x_{2},\cdots,x_{T})</annotation></semantics></math> denote the speech signal of an utterance, <math alttext="Y=(y_{1},y_{2},\cdots,y_{M})" class="ltx_Math" display="inline" id="S3.SS1.p1.m2" intent=":literal"><semantics><mrow><mi>Y</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo>,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msub><mi>y</mi><mi>M</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y=(y_{1},y_{2},\cdots,y_{M})</annotation></semantics></math> denote the ground-truth transcript of the utterance, and <math alttext="Z=(z_{1},z_{2},\cdots,z_{N})" class="ltx_Math" display="inline" id="S3.SS1.p1.m3" intent=":literal"><semantics><mrow><mi>Z</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>z</mi><mn>1</mn></msub><mo>,</mo><msub><mi>z</mi><mn>2</mn></msub><mo>,</mo><mi mathvariant="normal">⋯</mi><mo>,</mo><msub><mi>z</mi><mi>N</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Z=(z_{1},z_{2},\cdots,z_{N})</annotation></semantics></math> denote its corresponding text translation. The goal of the task is then to find the conditional distribution <math alttext="P(Z|X,Y)" class="ltx_Math" display="inline" id="S3.SS1.p1.m4" intent=":literal"><semantics><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(Z|X,Y)</annotation></semantics></math>. We hypothesize that often <math alttext="H(Z|X,Y)&lt;H(Z|Y)" class="ltx_Math" display="inline" id="S3.SS1.p1.m5" intent=":literal"><semantics><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>&lt;</mo><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mi>Y</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">H(Z|X,Y)&lt;H(Z|Y)</annotation></semantics></math> in practice, where <math alttext="H" class="ltx_Math" display="inline" id="S3.SS1.p1.m6" intent=":literal"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math> denotes the information entropy. In other words, the speech signal may contain additional information for a more accurate translation of the utterance, as it may be able to aid resolving ambiguities such as homographs, tonal variations, and omitted content—such as repetitions and filler words—that may be present in human-annotated transcripts.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p">In light of the remarkable performance observed with decoder-only language models in machine translation, we presume that encoder-decoder models’ audio-conditioned decoder possesses the potential for undertaking the audio-conditioned text translation task. In particular, one may prompt the decoder with source language text, generated either by human annotators or any ASR system, in the translation process, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S2.F1" title="Figure 1 ‣ 2.2 Multi-modal/-task Speech Systems ‣ 2 Related Work ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_tag">1</span></a>(b). Consequently, the resulting model is trained to learn the distribution <math alttext="P(Z|X,Y)" class="ltx_Math" display="inline" id="S3.SS1.p2.m1" intent=":literal"><semantics><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mrow><mi>X</mi><mo>,</mo><mi>Y</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(Z|X,Y)</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Translation with Speech-only Inputs</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p">The problem of speech translation can be directly modeled as <math alttext="P\left(Z|X\right)" class="ltx_Math" display="inline" id="S3.SS2.p1.m1" intent=":literal"><semantics><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo>)</mo></mrow></mrow><annotation encoding="application/x-tex">P\left(Z|X\right)</annotation></semantics></math> or modeled by marginalizing over an underlying latent variable, <math alttext="Y^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p1.m2" intent=":literal"><semantics><msup><mi>Y</mi><mo>′</mo></msup><annotation encoding="application/x-tex">Y^{\prime}</annotation></semantics></math>, representing valid transcripts of the audio <math alttext="X" class="ltx_Math" display="inline" id="S3.SS2.p1.m3" intent=":literal"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx1">
<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle P(Z|X)" class="ltx_Math" display="inline" id="S3.Ex1.m1" intent=":literal"><semantics><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle P(Z|X)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\sum_{Y^{\prime}}P(Z,Y^{\prime}|X)" class="ltx_Math" display="inline" id="S3.Ex1.m2" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><msup><mi>Y</mi><mo>′</mo></msup></munder></mstyle><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>Z</mi><mo>,</mo><mrow><msup><mi>Y</mi><mo>′</mo></msup><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\sum_{Y^{\prime}}P(Z,Y^{\prime}|X)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\sum_{Y^{\prime}}P(Z|Y^{\prime},X)P(Y^{\prime}|X)" class="ltx_Math" display="inline" id="S3.E1.m1" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><msup><mi>Y</mi><mo>′</mo></msup></munder></mstyle><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mrow><msup><mi>Y</mi><mo>′</mo></msup><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>Y</mi><mo>′</mo></msup><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\sum_{Y^{\prime}}P(Z|Y^{\prime},X)P(Y^{\prime}|X)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">However, the summation over <math alttext="Y^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p1.m4" intent=":literal"><semantics><msup><mi>Y</mi><mo>′</mo></msup><annotation encoding="application/x-tex">Y^{\prime}</annotation></semantics></math> is generally intractable. One common solution, also adopted by cascaded approaches to speech translation, is to approximate the summation with the single highest weight term in the summation, i.e.,</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx2">
<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\sum_{Y^{\prime}}P(Z|Y^{\prime},X)P(Y^{\prime}|X)" class="ltx_Math" display="inline" id="S3.Ex2.m1" intent=":literal"><semantics><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><msup><mi>Y</mi><mo>′</mo></msup></munder></mstyle><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mrow><msup><mi>Y</mi><mo>′</mo></msup><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>Y</mi><mo>′</mo></msup><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\sum_{Y^{\prime}}P(Z|Y^{\prime},X)P(Y^{\prime}|X)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright" colspan="2"></td>
</tr></tbody>
<tbody id="S3.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\approx\max_{Y^{\prime}}P(Z|Y^{\prime},X)" class="ltx_Math" display="inline" id="S3.Ex3.m1" intent=":literal"><semantics><mrow><mi></mi><mo>≈</mo><mrow><mrow><munder><mi>max</mi><msup><mi>Y</mi><mo>′</mo></msup></munder><mo lspace="0.167em">⁡</mo><mi>P</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mrow><msup><mi>Y</mi><mo>′</mo></msup><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\approx\max_{Y^{\prime}}P(Z|Y^{\prime},X)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle P(Y^{\prime}|X)," class="ltx_Math" display="inline" id="S3.Ex3.m2" intent=":literal"><semantics><mrow><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>Y</mi><mo>′</mo></msup><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle P(Y^{\prime}|X),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p">and furthermore to assume that the best transcript is the most likely one:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx3">
<tbody id="S3.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{Y}" class="ltx_Math" display="inline" id="S3.Ex4.m1" intent=":literal"><semantics><mover accent="true"><mi>Y</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\displaystyle\hat{Y}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\operatorname*{arg\,max}_{Y^{\prime}}P(Z|Y^{\prime},X)P(Y^{\prime}|X)" class="ltx_Math" display="inline" id="S3.Ex4.m2" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mrow><munder><mrow><mi>arg</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>max</mi></mrow><msup><mi>Y</mi><mo>′</mo></msup></munder><mo>⁡</mo><mi>P</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mrow><msup><mi>Y</mi><mo>′</mo></msup><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>Y</mi><mo>′</mo></msup><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=\operatorname*{arg\,max}_{Y^{\prime}}P(Z|Y^{\prime},X)P(Y^{\prime}|X)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\approx\operatorname*{arg\,max}_{Y^{\prime}}P(Y^{\prime}|X)." class="ltx_Math" display="inline" id="S3.E2.m1" intent=":literal"><semantics><mrow><mrow><mi></mi><mo>≈</mo><mrow><mrow><munder><mrow><mi>arg</mi><mo lspace="0.170em" rspace="0em">​</mo><mi>max</mi></mrow><msup><mi>Y</mi><mo>′</mo></msup></munder><mo>⁡</mo><mi>P</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>Y</mi><mo>′</mo></msup><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\approx\operatorname*{arg\,max}_{Y^{\prime}}P(Y^{\prime}|X).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p">However, cascaded speech translation further assumes that the translation is conditionally independent of the audio given the transcript,</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P(Z|\hat{Y},X)=P(Z|\hat{Y})," class="ltx_Math" display="block" id="S3.E3.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mrow><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mover accent="true"><mi>Y</mi><mo>^</mo></mover></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">P(Z|\hat{Y},X)=P(Z|\hat{Y}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which is practical in that it enables modular training of components, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="P(Z|X)=P(Z|\hat{Y})P(\hat{Y}|X)," class="ltx_Math" display="block" id="S3.E4.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mover accent="true"><mi>Y</mi><mo>^</mo></mover></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">P(Z|X)=P(Z|\hat{Y})P(\hat{Y}|X),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="P(Z|Y)" class="ltx_Math" display="inline" id="S3.SS2.p2.m1" intent=":literal"><semantics><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mi>Y</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(Z|Y)</annotation></semantics></math> and <math alttext="P(Y|X)" class="ltx_Math" display="inline" id="S3.SS2.p2.m2" intent=":literal"><semantics><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Y</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(Y|X)</annotation></semantics></math> can be trained separately, but it at the cost of a possible unneeded additional approximation.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p">End-to-end systems such as Whisper, however, model the problem without explicitly conditioning on the ASR transcripts, <math alttext="Y^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.p3.m1" intent=":literal"><semantics><msup><mi>Y</mi><mo>′</mo></msup><annotation encoding="application/x-tex">Y^{\prime}</annotation></semantics></math>. Its single-decoder multi-task paradigm presumably captures a higher-level abstract semantics of the speech signals, such that the ST decoding process is implicitly entangled with the model’s ASR ability.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p">We seek to combine the modeling advantages of the cascaded and end-to-end systems and generalize the multi-modal translation setting to re-formulate the system’s speech-only translation process for approximating Equation <a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S3.E1" title="In 3.2 Translation with Speech-only Inputs ‣ 3 Methodology ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_tag">1</span></a>. Specifically, we relax the conditional independence assumption of cascade approaches, by endowing end-to-end speech translation models with the capacity to also condition on either a ground-truth or hypothesized transcript defined by Equation <a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S3.E2" title="In 3.2 Translation with Speech-only Inputs ‣ 3 Methodology ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_tag">2</span></a>, i.e.:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx4">
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle P(Z|X)" class="ltx_Math" display="inline" id="S3.E5.m1" intent=":literal"><semantics><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\displaystyle P(Z|X)</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=P(Z|\hat{Y},X)P(\hat{Y}|X)" class="ltx_Math" display="inline" id="S3.E5.m2" intent=":literal"><semantics><mrow><mi></mi><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Z</mi><mo fence="false">|</mo><mrow><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mo>,</mo><mi>X</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mover accent="true"><mi>Y</mi><mo>^</mo></mover><mo fence="false">|</mo><mi>X</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle=P(Z|\hat{Y},X)P(\hat{Y}|X)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p">In our implementation, we carry out a <span class="ltx_text ltx_font_bold">two-stage decoding</span> process. In the first stage, the model is used to produce the ASR hypotheses, and subsequently, in the second stage, the model conditions on them to generate the translations.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p">An alternative perspective on this modeling is that it fully leverages the system’s source-language modeling capability. In end-to-end multi-task models, the decoder can be viewed as implicitly “partitioned” into two roles: source-language modeling and target-language generation. While these functions share parameters and benefit from joint optimization, they may still develop distinct competencies. By conditioning translation on both speech and textual transcripts, this approach explicitly harnesses a well-trained source-language model—potentially even from an external ASR system—allowing the decoder to generate more accurate translations. This perspective highlights how multi-modal conditioning can serve as a mechanism to refine and reinforce the system’s understanding of the source language, ultimately improving translation quality.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Translation with Text-only Inputs</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p">Integrating MT functionality into a multi-modal encoder-decoder model presents unique challenges. In conventional encoder-decoder MT systems, the source language text is processed through the encoder, which generates contextual representations for the decoder to cross-attend to. However, oftentimes the pre-trained encoder is designed specifically for processing speech features, making direct text encoding potentially ineffective. Training the encoder to handle text inputs would require a significant amount of additional data and could lead to catastrophic forgetting, where the model loses its ability to process speech effectively.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p">Inspired by the success of decoder-only MT models such as GPT-like systems, we adopt an alternative strategy: instead of modifying the encoder to accommodate text, we encode the source text directly within the decoder, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S2.F1" title="Figure 1 ‣ 2.2 Multi-modal/-task Speech Systems ‣ 2 Related Work ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_tag">1</span></a>(a). Specifically, we prepend the source text as a prefix to the decoder input, leveraging the self-attention mechanism to implicitly model source-target dependencies. However, implementing this method within an encoder-decoder framework requires careful handling of the cross-attention mechanism. Since the decoder in our system is designed to attend to encoded speech representations, directly bypassing the encoder would disrupt the model’s expected structure. To address this, we introduce a single learnable vector in the encoder, serving as an indicator that informs the decoder that text input is being processed. The remaining encoder output is padded with zeros, and we modify the cross-attention mask such that the decoder attends only to this learnable embedding. This design ensures that the model’s architecture remains structurally intact while effectively repurposing the decoder for text-based translation.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Whisper-UT: Unified Translation System</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p">To achieve a unified translation framework that encompasses multiple translation paradigms, we propose Whisper-UT, a system designed to handle ASR, ST, MT, and MMT within a single model. Our approach is built on multi-task learning, leveraging 3-way parallel data and text-only MT data to optimize multiple objectives in a stochastic fashion.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>3-way Parallel Data Objectives</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p">We formulate the learning process with six distinct training objectives, categorized based on the availability of parallel data.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p2">
<p class="ltx_p">For the 3-way dataset that provide speech, transcripts, and translations <math alttext="\{X,Y,Z\}" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p2.m1" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mi>X</mi><mo>,</mo><mi>Y</mi><mo>,</mo><mi>Z</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{X,Y,Z\}</annotation></semantics></math>, we define three primary objectives:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">ASR Objective.</span> Learning the mapping <math alttext="X\to Y" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p3.m1" intent=":literal"><semantics><mrow><mi>X</mi><mo stretchy="false">→</mo><mi>Y</mi></mrow><annotation encoding="application/x-tex">X\to Y</annotation></semantics></math>, i.e., predicting the source language transcript from speech.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">E2E-ST Objective.</span> Directly predicting the target language text <math alttext="Z" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p4.m1" intent=":literal"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math> from speech <math alttext="X" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p4.m2" intent=":literal"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p5">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">MMT Objective.</span> Predicting <math alttext="Z" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p5.m1" intent=":literal"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math> while attending to both <math alttext="X" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p5.m2" intent=":literal"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math> (speech) and <math alttext="Y" class="ltx_Math" display="inline" id="S3.SS4.SSS1.p5.m3" intent=":literal"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math> (source transcript).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Text-Only Data Objectives</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p">Since 3-way parallel datasets are scarce in reality, we incorporate text-only MT data <math alttext="\{Y,Z\}" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mi>Y</mi><mo>,</mo><mi>Z</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{Y,Z\}</annotation></semantics></math> and define additional objectives:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Source Language Modeling (SLM):</span> Predicting the next source token in <math alttext="Y" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p2.m1" intent=":literal"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>, acting as an ASR surrogate for text-only samples.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Target Language Modeling (TLM):</span> Predicting the next token in <math alttext="Z" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p3.m1" intent=":literal"><semantics><mi>Z</mi><annotation encoding="application/x-tex">Z</annotation></semantics></math>, improving the decoder’s target language modeling ability.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p4">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">MT:</span> Translating <math alttext="Y\to Z" class="ltx_Math" display="inline" id="S3.SS4.SSS2.p4.m1" intent=":literal"><semantics><mrow><mi>Y</mi><mo stretchy="false">→</mo><mi>Z</mi></mrow><annotation encoding="application/x-tex">Y\to Z</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p5">
<p class="ltx_p">For MMT and MT objectives, we allow gradients to propagate back through the source language tokens, implicitly enhancing the model’s source language modeling ability.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Dynamic Loss Weighting</h4>
<div class="ltx_para" id="S3.SS4.SSS3.p1">
<p class="ltx_p">To balance the competing objectives, we employ a <span class="ltx_text ltx_font_bold">stochastic task selection mechanism</span> with beta-distributed loss weighting inspired by <cite class="ltx_cite ltx_citemacro_cite">Zhang and Patel (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib41" title="">2024</a>)</cite>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx5">
<tbody id="S3.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\alpha\sim\text{Beta}(\beta_{1},\beta_{2})," class="ltx_Math" display="inline" id="S3.E6.m1" intent=":literal"><semantics><mrow><mrow><mi>α</mi><mo>∼</mo><mrow><mtext>Beta</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>β</mi><mn>1</mn></msub><mo>,</mo><msub><mi>β</mi><mn>2</mn></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\alpha\sim\text{Beta}(\beta_{1},\beta_{2}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">which determines the final multi-task loss:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx6">
<tbody id="S3.E7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{\text{mtl}}" class="ltx_Math" display="inline" id="S3.E7.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>mtl</mtext></msub><annotation encoding="application/x-tex">\displaystyle\mathcal{L}_{\text{mtl}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(1-\alpha)\mathcal{L}^{CE}_{\text{asr}}+\alpha\mathcal{L}^{CE}_{\text{st}}," class="ltx_Math" display="inline" id="S3.E7.m2" intent=":literal"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>α</mi></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><msubsup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>asr</mtext><mrow><mi>C</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msubsup></mrow><mo>+</mo><mrow><mi>α</mi><mo lspace="0em" rspace="0em">​</mo><msubsup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>st</mtext><mrow><mi>C</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msubsup></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=(1-\alpha)\mathcal{L}^{CE}_{\text{asr}}+\alpha\mathcal{L}^{CE}_{\text{st}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p">where <math alttext="\mathcal{L}^{CE}_{\text{asr}}" class="ltx_Math" display="inline" id="S3.SS4.SSS3.p1.m1" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>asr</mtext><mrow><mi>C</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msubsup><annotation encoding="application/x-tex">\mathcal{L}^{CE}_{\text{asr}}</annotation></semantics></math> is the ASR loss (or SLM loss for text-only samples), and <math alttext="\mathcal{L}^{CE}_{\text{st}}" class="ltx_Math" display="inline" id="S3.SS4.SSS3.p1.m2" intent=":literal"><semantics><msubsup><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>st</mtext><mrow><mi>C</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msubsup><annotation encoding="application/x-tex">\mathcal{L}^{CE}_{\text{st}}</annotation></semantics></math> is either the ST loss or the MMT loss, selected via stochastic task selection.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS3.p2">
<p class="ltx_p">The stochastic weighting scheme is motivated by empirical findings that equal task weighting leads to gradient interference, degrading performance across tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.4 </span>Utterance-Level Task Selection</h4>
<div class="ltx_para" id="S3.SS4.SSS4.p1">
<p class="ltx_p">Each batch is sampled from a mixture of the 3-way parallel data and text-only MT data. We define the loss computation as follows:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">ASR Loss:</span> Always computed for speech-based samples; replaced with SLM loss for text-only samples (zero-padded input except for a learnable vector).</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">ST vs. MMT Objective:</span>
With probability <math alttext="q" class="ltx_Math" display="inline" id="S3.I1.i2.p1.m1" intent=":literal"><semantics><mi>q</mi><annotation encoding="application/x-tex">q</annotation></semantics></math>, apply standard ST loss; for text-only data, this is equivalent to the TLM loss.
With probability <math alttext="(1-q)" class="ltx_Math" display="inline" id="S3.I1.i2.p1.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>q</mi></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(1-q)</annotation></semantics></math>, apply MMT loss, where the decoder cross-attends to both speech features and source text tokens; for text-only data, this becomes the conventional MT loss.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.5 </span>Error Simulation in Multi-Modal Translation</h4>
<div class="ltx_para" id="S3.SS4.SSS5.p1">
<p class="ltx_p">For MMT, we introduce an ASR error simulation mechanism to enhance robustness. With probability <math alttext="b" class="ltx_Math" display="inline" id="S3.SS4.SSS5.p1.m1" intent=":literal"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>, we perturb a batch by replacing the source language tokens, sampled with probability <math alttext="t" class="ltx_Math" display="inline" id="S3.SS4.SSS5.p1.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, with a similar alternative sampled randomly from the top-<math alttext="k" class="ltx_Math" display="inline" id="S3.SS4.SSS5.p1.m3" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> nearest neighbors in the embeddings space. To explicitly signal perturbed inputs, we prepend a special token to the modified sequence, allowing for the model to dynamically re-weight its reliance on the noisy text prefix and the corresponding audio input at inference time. This aims to simulate real-world noise in transcripts (e.g., ASR errors, omissions), encouraging the model to rely on both modalities for translation.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Unified Training Framework</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p">In summary, our unified training framework integrates ASR, ST, MMT, and MT into a single multi-task learning process. To achieve this, we first concatenate both speech-text and text-only datasets, allowing for random sampling within each batch. For every batch, we compute the ASR loss, which corresponds to the source language modeling loss when dealing with text-only samples. The ASR and ST loss weights are dynamically balanced by sampling a weight <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS5.p1.m1" intent=":literal"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math> from a Beta distribution. Next, we stochastically determine whether the batch follows the ST/TLM objective or the MMT/MT objective. If the batch is selected for MMT training, ASR error simulation is applied with a certain probability to mimic transcription imperfections and enhance robustness. By combining these components, Whisper-UT serves as a unified model for ASR, ST, MT, and MMT, leveraging both textual and speech inputs efficiently.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Tasks and Datasets</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p">We test our approach on CoVoST2, a general-domain speech translation benchmark, using its French-English (180 hours) and German-English (119 hours) subsets for training. To assess performance on challenging conversational telephony speech (CTS), we conduct experiments on the Fisher-CallHome Spanish-to-English corpus (186 hours of spontaneous Spanish dialogues) and the BBN Mandarin-to-English corpus (110 hours of Mandarin-English telephony conversations). This setup tests our method’s adaptability across both general and domain-specific speech, with CTS posing unique challenges such as disfluencies, code-switching, and informal dialogue structures.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p">For both ASR and ST, we normalize the text by lower-casing all characters and removing all punctuations before computing the metrics. For the Fisher Spanish corpus, the BLEU score is computed with multiple references using the Moses <cite class="ltx_cite ltx_citemacro_cite">Koehn et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib16" title="">2007</a>)</cite> toolkit as reported in other work <cite class="ltx_cite ltx_citemacro_cite">Weiss et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib37" title="">2017a</a>)</cite>. The evaluation script used is provided in the code.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Training</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p">To demonstrate our proposed approach, we adopt the <span class="ltx_text ltx_font_smallcaps">large-v2</span> version of Whisper with 1.6 billion parameters as the base model and fine-tune it for our unified translation modeling.
To enable joint training of speech-to-text and text-to-text translation within a single framework, we repurpose the 3-way parallel dataset by strategically replicating its text pairs. Specifically, we create a duplicate of the original dataset where the audio signals are removed, retaining only the source-target text pairs. This allows us to simulate text-only data without introducing external resources, ensuring parity in training scale across objectives.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Experimental Results</h3>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Direct Whisper fine-tuning results on the Fisher-Spanish and BBN-Mandarin datasets. The <span class="ltx_text ltx_font_bold">Objective</span> column specifies under which training objective the model system is fine-tuned. <span class="ltx_text ltx_font_italic">None</span> refers to the original model. <span class="ltx_text ltx_framed ltx_framed_underline">Underline</span> highlights the cross-task synergy.
</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Objective</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Task</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">ASR</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">E2E-ST</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">(WER<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">(BLEU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.m2" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Fisher</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">None</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="26.7" class="ltx_Math" display="inline" id="S4.T1.m3" intent=":literal"><semantics><mn>26.7</mn><annotation encoding="application/x-tex">26.7</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="51.6" class="ltx_Math" display="inline" id="S4.T1.m4" intent=":literal"><semantics><mn>51.6</mn><annotation encoding="application/x-tex">51.6</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ASR</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="19.1" class="ltx_Math" display="inline" id="S4.T1.m5" intent=":literal"><semantics><mn>19.1</mn><annotation encoding="application/x-tex">19.1</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline"><math alttext="54.9" class="ltx_Math" display="inline" id="S4.T1.m6" intent=":literal"><semantics><mn>54.9</mn><annotation encoding="application/x-tex">54.9</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ST</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline"><math alttext="20.3" class="ltx_Math" display="inline" id="S4.T1.m7" intent=":literal"><semantics><mn>20.3</mn><annotation encoding="application/x-tex">20.3</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="61.2" class="ltx_Math" display="inline" id="S4.T1.m8" intent=":literal"><semantics><mn>61.2</mn><annotation encoding="application/x-tex">61.2</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">4</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">BBN</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">None</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="32.2" class="ltx_Math" display="inline" id="S4.T1.m9" intent=":literal"><semantics><mn>32.2</mn><annotation encoding="application/x-tex">32.2</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="13.0" class="ltx_Math" display="inline" id="S4.T1.m10" intent=":literal"><semantics><mn>13.0</mn><annotation encoding="application/x-tex">13.0</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ASR</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="{18.9}" class="ltx_Math" display="inline" id="S4.T1.m11" intent=":literal"><semantics><mn>18.9</mn><annotation encoding="application/x-tex">{18.9}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline"><math alttext="16.2" class="ltx_Math" display="inline" id="S4.T1.m12" intent=":literal"><semantics><mn>16.2</mn><annotation encoding="application/x-tex">16.2</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ST</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline"><math alttext="23.1" class="ltx_Math" display="inline" id="S4.T1.m13" intent=":literal"><semantics><mn>23.1</mn><annotation encoding="application/x-tex">23.1</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="{16.8}" class="ltx_Math" display="inline" id="S4.T1.m14" intent=":literal"><semantics><mn>16.8</mn><annotation encoding="application/x-tex">{16.8}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Overview</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p">Table <a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4.T1" title="Table 1 ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_tag">1</span></a> presents results from directly fine-tuning Whisper, which reveals a cross-task synergy phenomenon: optimizing for one task (e.g., ASR) not only preserves but often enhances performance on another (e.g., ST), as indicated by underlined improvements across both datasets.
Table <a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4.T2" title="Table 2 ‣ 4.4.1 Overview ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_tag">2</span></a> reports Whisper-UT results on three corpora: CoVoST2 (French <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.m1" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math> English, German <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p1.m2" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math> English), Fisher‑Spanish, and BBN‑Mandarin. Across all settings, our proposed Whisper‑UT variants demonstrate consistent improvements in transcription accuracy (WER↓) and translation quality (BLEU↑).</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>
Results on the test sets.
<span class="ltx_text ltx_font_italic">MMT</span> refers to the translation process that conditions on both the ground-truth transcript and the speech signals, while <span class="ltx_text ltx_font_italic">2-Stage-ST</span> refers to the MMT process with ASR hypothesis.
</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Task</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Task</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Metrics</span></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Results</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="12" style="padding-left:5.0pt;padding-right:5.0pt;">ASR</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_inline-block">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">CoVoST2</span></span>
<span class="ltx_p"><span class="ltx_text ltx_font_italic">fr-en</span> | <span class="ltx_text ltx_font_italic">de-en</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Baseline <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib36" title="">2020b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3" style="padding-left:5.0pt;padding-right:5.0pt;">WER<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="18.3" class="ltx_Math" display="inline" id="S4.T2.m2" intent=":literal"><semantics><mn>18.3</mn><annotation encoding="application/x-tex">18.3</annotation></semantics></math> | <math alttext="21.4" class="ltx_Math" display="inline" id="S4.T2.m3" intent=":literal"><semantics><mn>21.4</mn><annotation encoding="application/x-tex">21.4</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-Large-V2</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="13.4" class="ltx_Math" display="inline" id="S4.T2.m4" intent=":literal"><semantics><mn>13.4</mn><annotation encoding="application/x-tex">13.4</annotation></semantics></math> | <math alttext="7.0" class="ltx_Math" display="inline" id="S4.T2.m5" intent=":literal"><semantics><mn>7.0</mn><annotation encoding="application/x-tex">7.0</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-UT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="\mathbf{8.3}" class="ltx_Math" display="inline" id="S4.T2.m6" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">8.3</mn><annotation encoding="application/x-tex">\mathbf{8.3}</annotation></semantics></math> | <math alttext="\mathbf{5.8}" class="ltx_Math" display="inline" id="S4.T2.m7" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">5.8</mn><annotation encoding="application/x-tex">\mathbf{5.8}</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="6" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Fisher-Spanish</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">SeamlessM4T-Large</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="6" style="padding-left:5.0pt;padding-right:5.0pt;">WER<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.m8" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="76.3" class="ltx_Math" display="inline" id="S4.T2.m9" intent=":literal"><semantics><mn>76.3</mn><annotation encoding="application/x-tex">76.3</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-Large-V2</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="26.7" class="ltx_Math" display="inline" id="S4.T2.m10" intent=":literal"><semantics><mn>26.7</mn><annotation encoding="application/x-tex">26.7</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Seq2seq <cite class="ltx_cite ltx_citemacro_cite">Weiss et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib38" title="">2017b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="23.2" class="ltx_Math" display="inline" id="S4.T2.m11" intent=":literal"><semantics><mn>23.2</mn><annotation encoding="application/x-tex">23.2</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Multi-ASR <cite class="ltx_cite ltx_citemacro_cite">Inaguma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib13" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="22.9" class="ltx_Math" display="inline" id="S4.T2.m12" intent=":literal"><semantics><mn>22.9</mn><annotation encoding="application/x-tex">22.9</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">STAC-ST <cite class="ltx_cite ltx_citemacro_cite">Zuluaga-Gomez et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib43" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="18.8" class="ltx_Math" display="inline" id="S4.T2.m13" intent=":literal"><semantics><mn>18.8</mn><annotation encoding="application/x-tex">18.8</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">9</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-UT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{16.3}" class="ltx_Math" display="inline" id="S4.T2.m14" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">16.3</mn><annotation encoding="application/x-tex">\mathbf{16.3}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">10</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">BBN-Mandarin</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">SeamlessM4T-Large</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3" style="padding-left:5.0pt;padding-right:5.0pt;">WER<math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.m15" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="52.6" class="ltx_Math" display="inline" id="S4.T2.m16" intent=":literal"><semantics><mn>52.6</mn><annotation encoding="application/x-tex">52.6</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">11</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-Large-V2</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="32.2" class="ltx_Math" display="inline" id="S4.T2.m17" intent=":literal"><semantics><mn>32.2</mn><annotation encoding="application/x-tex">32.2</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">12</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-UT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{17.4}" class="ltx_Math" display="inline" id="S4.T2.m18" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">17.4</mn><annotation encoding="application/x-tex">\mathbf{17.4}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">13</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="8" style="padding-left:5.0pt;padding-right:5.0pt;">MT</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_inline-block">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">CoVoST2</span></span>
<span class="ltx_p"><span class="ltx_text ltx_font_italic">fr-en</span> | <span class="ltx_text ltx_font_italic">de-en</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Baseline <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib36" title="">2020b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3" style="padding-left:5.0pt;padding-right:5.0pt;">BLEU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.m19" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="37.9" class="ltx_Math" display="inline" id="S4.T2.m20" intent=":literal"><semantics><mn>37.9</mn><annotation encoding="application/x-tex">37.9</annotation></semantics></math> | <math alttext="28.2" class="ltx_Math" display="inline" id="S4.T2.m21" intent=":literal"><semantics><mn>28.2</mn><annotation encoding="application/x-tex">28.2</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">14</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">NLLB-1.3B</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="\mathbf{42.3}" class="ltx_Math" display="inline" id="S4.T2.m22" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">42.3</mn><annotation encoding="application/x-tex">\mathbf{42.3}</annotation></semantics></math> | <math alttext="\mathbf{31.0}" class="ltx_Math" display="inline" id="S4.T2.m23" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">31.0</mn><annotation encoding="application/x-tex">\mathbf{31.0}</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">15</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-UT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="36.5" class="ltx_Math" display="inline" id="S4.T2.m24" intent=":literal"><semantics><mn>36.5</mn><annotation encoding="application/x-tex">36.5</annotation></semantics></math> | <math alttext="26.9" class="ltx_Math" display="inline" id="S4.T2.m25" intent=":literal"><semantics><mn>26.9</mn><annotation encoding="application/x-tex">26.9</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">16</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_inline-block">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Fisher-Spanish</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">NLLB-1.3B</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3" style="padding-left:5.0pt;padding-right:5.0pt;">BLEU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.m26" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="48.3" class="ltx_Math" display="inline" id="S4.T2.m27" intent=":literal"><semantics><mn>48.3</mn><annotation encoding="application/x-tex">48.3</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">17</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Bi-NMT <cite class="ltx_cite ltx_citemacro_cite">Inaguma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib13" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{59.6}" class="ltx_Math" display="inline" id="S4.T2.m28" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">59.6</mn><annotation encoding="application/x-tex">\mathbf{59.6}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">18</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-UT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="55.9" class="ltx_Math" display="inline" id="S4.T2.m29" intent=":literal"><semantics><mn>55.9</mn><annotation encoding="application/x-tex">55.9</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">19</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="2" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_inline-block">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">BBN-Mandarin</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">NLLB-1.3B</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="2" style="padding-left:5.0pt;padding-right:5.0pt;">BLEU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.m30" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="8.7" class="ltx_Math" display="inline" id="S4.T2.m31" intent=":literal"><semantics><mn>8.7</mn><annotation encoding="application/x-tex">8.7</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">20</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-UT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{15.7}" class="ltx_Math" display="inline" id="S4.T2.m32" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">15.7</mn><annotation encoding="application/x-tex">\mathbf{15.7}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">21</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="3" style="padding-left:5.0pt;padding-right:5.0pt;">MMT</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_inline-block">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">CoVoST2</span></span>
<span class="ltx_p"><span class="ltx_text ltx_font_italic">fr-en</span> | <span class="ltx_text ltx_font_italic">de-en</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-UT</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">BLEU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.m33" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="\mathbf{46.2}" class="ltx_Math" display="inline" id="S4.T2.m34" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">46.2</mn><annotation encoding="application/x-tex">\mathbf{46.2}</annotation></semantics></math> | <math alttext="\mathbf{40.1}" class="ltx_Math" display="inline" id="S4.T2.m35" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">40.1</mn><annotation encoding="application/x-tex">\mathbf{40.1}</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">22</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Fisher-Spanish</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-UT</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">BLEU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.m36" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{70.4}" class="ltx_Math" display="inline" id="S4.T2.m37" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">70.4</mn><annotation encoding="application/x-tex">\mathbf{70.4}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">23</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">BBN-Mandarin</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-UT</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">BLEU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.m38" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{26.0}" class="ltx_Math" display="inline" id="S4.T2.m39" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">26.0</mn><annotation encoding="application/x-tex">\mathbf{26.0}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">24</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="18" style="padding-left:5.0pt;padding-right:5.0pt;">ST</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="6" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_inline-block">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">CoVoST2</span></span>
<span class="ltx_p"><span class="ltx_text ltx_font_italic">fr-en</span> | <span class="ltx_text ltx_font_italic">de-en</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">Baseline <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib36" title="">2020b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="6" style="padding-left:5.0pt;padding-right:5.0pt;">BLEU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.m40" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="27.6" class="ltx_Math" display="inline" id="S4.T2.m41" intent=":literal"><semantics><mn>27.6</mn><annotation encoding="application/x-tex">27.6</annotation></semantics></math> | <math alttext="21.0" class="ltx_Math" display="inline" id="S4.T2.m42" intent=":literal"><semantics><mn>21.0</mn><annotation encoding="application/x-tex">21.0</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">25</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">SeamlessM4T-Large</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="33.1" class="ltx_Math" display="inline" id="S4.T2.m43" intent=":literal"><semantics><mn>33.1</mn><annotation encoding="application/x-tex">33.1</annotation></semantics></math> | <math alttext="35.8" class="ltx_Math" display="inline" id="S4.T2.m44" intent=":literal"><semantics><mn>35.8</mn><annotation encoding="application/x-tex">35.8</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">26</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-Large-V2</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="36.7" class="ltx_Math" display="inline" id="S4.T2.m45" intent=":literal"><semantics><mn>36.7</mn><annotation encoding="application/x-tex">36.7</annotation></semantics></math> | <math alttext="36.8" class="ltx_Math" display="inline" id="S4.T2.m46" intent=":literal"><semantics><mn>36.8</mn><annotation encoding="application/x-tex">36.8</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">27</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">QWen2-Audio <cite class="ltx_cite ltx_citemacro_cite">Chu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib6" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="38.5" class="ltx_Math" display="inline" id="S4.T2.m47" intent=":literal"><semantics><mn>38.5</mn><annotation encoding="application/x-tex">38.5</annotation></semantics></math> | <math alttext="35.2" class="ltx_Math" display="inline" id="S4.T2.m48" intent=":literal"><semantics><mn>35.2</mn><annotation encoding="application/x-tex">35.2</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">28</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-UT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="{40.8}" class="ltx_Math" display="inline" id="S4.T2.m49" intent=":literal"><semantics><mn>40.8</mn><annotation encoding="application/x-tex">{40.8}</annotation></semantics></math> | <math alttext="{37.7}" class="ltx_Math" display="inline" id="S4.T2.m50" intent=":literal"><semantics><mn>37.7</mn><annotation encoding="application/x-tex">{37.7}</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">29</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-UT-<span class="ltx_text ltx_font_italic">2-Stage</span>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">
<math alttext="\mathbf{41.4}" class="ltx_Math" display="inline" id="S4.T2.m51" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">41.4</mn><annotation encoding="application/x-tex">\mathbf{41.4}</annotation></semantics></math> | <math alttext="\mathbf{38.1}" class="ltx_Math" display="inline" id="S4.T2.m52" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">38.1</mn><annotation encoding="application/x-tex">\mathbf{38.1}</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">30</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="7" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Fisher-Spanish</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">SeamlessM4T-Large</td>
<td class="ltx_td ltx_align_center ltx_border_t" rowspan="7" style="padding-left:5.0pt;padding-right:5.0pt;">BLEU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.m53" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="14.7" class="ltx_Math" display="inline" id="S4.T2.m54" intent=":literal"><semantics><mn>14.7</mn><annotation encoding="application/x-tex">14.7</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">31</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Multi-ST <cite class="ltx_cite ltx_citemacro_cite">Inaguma et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib13" title="">2019</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="45.2" class="ltx_Math" display="inline" id="S4.T2.m55" intent=":literal"><semantics><mn>45.2</mn><annotation encoding="application/x-tex">45.2</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">32</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Multi-task ST/ASR <cite class="ltx_cite ltx_citemacro_cite">Weiss et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib38" title="">2017b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="48.7" class="ltx_Math" display="inline" id="S4.T2.m56" intent=":literal"><semantics><mn>48.7</mn><annotation encoding="application/x-tex">48.7</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">33</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-Large-V2</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="51.6" class="ltx_Math" display="inline" id="S4.T2.m57" intent=":literal"><semantics><mn>51.6</mn><annotation encoding="application/x-tex">51.6</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">34</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">STAC-ST <cite class="ltx_cite ltx_citemacro_cite">Zuluaga-Gomez et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib43" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="52.6" class="ltx_Math" display="inline" id="S4.T2.m58" intent=":literal"><semantics><mn>52.6</mn><annotation encoding="application/x-tex">52.6</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">35</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-UT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="{62.0}" class="ltx_Math" display="inline" id="S4.T2.m59" intent=":literal"><semantics><mn>62.0</mn><annotation encoding="application/x-tex">{62.0}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">36</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-UT-<span class="ltx_text ltx_font_italic">2-Stage</span>
</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{62.1}" class="ltx_Math" display="inline" id="S4.T2.m60" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">62.1</mn><annotation encoding="application/x-tex">\mathbf{62.1}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">37</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="4" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">BBN-Mandarin</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">SeamlessM4T-Large</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="4" style="padding-left:5.0pt;padding-right:5.0pt;">BLEU<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.m61" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="7.0" class="ltx_Math" display="inline" id="S4.T2.m62" intent=":literal"><semantics><mn>7.0</mn><annotation encoding="application/x-tex">7.0</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">38</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-Large-V2</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="13.0" class="ltx_Math" display="inline" id="S4.T2.m63" intent=":literal"><semantics><mn>13.0</mn><annotation encoding="application/x-tex">13.0</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">39</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-UT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="19.8" class="ltx_Math" display="inline" id="S4.T2.m64" intent=":literal"><semantics><mn>19.8</mn><annotation encoding="application/x-tex">19.8</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">40</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper-UT-<span class="ltx_text ltx_font_italic">2-Stage</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{21.6}" class="ltx_Math" display="inline" id="S4.T2.m65" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">21.6</mn><annotation encoding="application/x-tex">\mathbf{21.6}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Cross-task Synergy</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p">Table <a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4.T1" title="Table 1 ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_tag">1</span></a> reveals that fine-tuning on one task does not only improve performance on the target task but also benefits other tasks as well. Notably, ASR fine-tuning enhances ST performance (51.6 to 54.9 on Fisher and 13.0 to 16.2 on BBN), and ST fine-tuning reciprocally benefits ASR (26.7 to 20.3 on Fisher and 32.2 to 23.1 on BBN).
This suggests that cross-task fine-tuning may mutually reinforce capabilities without architectural changes, inspiring Whisper-UT’s unified speech-text framework.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3 </span>ASR</h4>
<div class="ltx_para" id="S4.SS4.SSS3.p1">
<p class="ltx_p">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4.T2" title="Table 2 ‣ 4.4.1 Overview ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_tag">2</span></a>, on CoVoST2, Whisper‑UT reduces WER from 13.4/7.0 (Whisper) to 8.3/5.8. Similar gains appear on Fisher (from 18.8 to 16.3) and BBN (from 32.2 to 17.4). These improvements suggest that our stochastic task-interleaving mechanism effectively mitigates catastrophic forgetting, despite the addition of MT and MMT as new tasks. This stability preserves modality-specific expertise while introducing new tasks and enabling cross-task synergy.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.4 </span>MT</h4>
<div class="ltx_para" id="S4.SS4.SSS4.p1">
<p class="ltx_p">In text-only translation, Whisper-UT—trained without architectural modifications—narrowly trails the 1.3B-parameter NLLB model on general-domain CoVoST2 (36.5/26.9 vs. 42.3/31.0 BLEU) but surpasses it by +7.6 and +7.0 BLEU on domain-specific Fisher-Spanish (55.9 vs. 48.3) and BBN-Mandarin (15.7 vs. 8.7) benchmarks, despite using fewer parameters and no dedicated MT pretraining. This divergence highlights two key insights: (1) Whisper’s decoder inherently functions as a multilingual language model, capable of text-to-text translation with light-touch adaptation, and (2) its cross-lingual transfer capabilities, honed during speech-centric pretraining, generalize robustly to textual MT in low-resource, domain-specific scenarios. Critically, these results validate our hypothesis that minimal modifications—enabling joint training on speech and text—can unlock Whisper’s latent capacity for unified cross-modal translation, bridging the gap between speech and text without sacrificing architectural simplicity.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.5 </span>MMT</h4>
<div class="ltx_para" id="S4.SS4.SSS5.p1">
<p class="ltx_p">When translating with access to both speech and ground‐truth transcripts, Whisper‑UT achieves 46.2/40.1 BLEU on CoVoST2, 70.4 BLEU on Fisher‑Spanish, and 26.0 BLEU on BBN‑Mandarin—surpassing all MT baselines. This substantial improvement underscores the complementary nature of audio and text modalities: acoustic cues (e.g., prosody, emotion, pauses, repetitions) resolve ambiguities in noisy transcripts, while lexical context sharpens alignment of speech-derived semantics. By explicitly modeling these mutually compensatory signals, our unified architecture fuses audio and text modalities, yielding more robust translations when multi-modal information is available.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.6 </span>ST</h4>
<div class="ltx_para" id="S4.SS4.SSS6.p1">
<p class="ltx_p">In the ST setting, Whisper-UT achieves competitive performance with single-pass end-to-end decoding: 40.8/37.7 BLEU on CoVoST2 (<span class="ltx_text ltx_font_typewriter">fr-en</span>/<span class="ltx_text ltx_font_typewriter">de-en</span>), 62.0 BLEU on Fisher-Spanish, and 19.8 BLEU on BBN-Mandarin, surpassing QWen2-Audio, SeamlessM4T, and STAC-ST by margins of 2–8 BLEU points. Crucially, the 2-Stage inference variant yields systematic improvements over promptless decoding: +0.6/+0.4 BLEU on CoVoST2 (41.4/38.1 vs. 40.8/37.7), +0.1 BLEU on Fisher-Spanish (62.1 vs. 62.0), and +1.8 BLEU on BBN-Mandarin (21.6 vs. 19.8). These improvements are amplified in error-prone conditions, reflecting successful mitigation of ASR error propagation—a key challenge in cascaded systems. By prepending the special token during training (with simulated ASR noise) and inference (for 2-Stage decoding), the model learns to conditionally distrust imperfect transcripts while retaining their partial utility, rebalancing reliance on audio signals to correct latent errors. These consistent incremental gains validate the effectiveness of our two-stage modeling, demonstrating that even imperfect intermediate transcripts enhance translation fidelity through explicit cross-modal grounding when combined with learned distrust mechanisms.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.7 </span>Summary</h4>
<div class="ltx_para" id="S4.SS4.SSS7.p1">
<p class="ltx_p">The unified Whisper-UT framework achieves robust performance across three key tasks: monolingual ASR, text-only machine translation, and speech translation. Improvements are most pronounced in conversational Mandarin and Spanish settings. Moreover, the 2‑Stage decoding strategy provides a reliable way to enhance translation in fully end‐to‐end deployments. Overall, these results highlight Whisper-UT’s ability to unify cross-modal and cross-lingual speech-text tasks within a single architecture, offering a versatile solution for scenarios requiring joint speech-text modeling.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p">In this paper, we introduced Whisper-UT, a unified translation framework that integrates ASR, ST, MT, and MMT within a single multi-task learning paradigm. In addition to this unified framework, we propose an explicit modeling approach for speech translation that conditions on both speech signals and textual prompts, effectively leveraging ASR hypotheses or ground-truth transcripts. Our training strategy, incorporating stochastic task selection and modality-aware error simulation, ensures effective multi-task learning while mitigating catastrophic forgetting. Experimental results show that Whisper-UT achieves strong performance across various translation tasks, demonstrating the benefits of cross-task synergy. Future work will explore scaling to more languages and extending to broader multi-modal scenarios.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations and Ethical Considerations</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p">While our approach demonstrates strong improvements, several limitations remain. To ensure fair comparisons, we kept training steps consistent across models, meaning our best-performing system may not have reached its full potential with extended training.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p">Due to resource constraints, we fine-tuned Whisper rather than training from scratch, which might limit the full integration of the objectives. Ideally, to demonstrate cross-task fine-tuning, we would start from a pretrained model that natively support each of our tasks, (MT, MMT, ST, ASR), but building state-of-the-art, or close to state-of-the-art systems requires building from existing models, such as Whisper, and adapting to Whisper to additionally perform these tasks, while a contribution in its own right, ultimately requires a two-stage fine-tuning approach that complicates analysis of the effectiveness of cross-task fine-tuning. Furthermore, while we believe our method to be general, i.e., it could be applied to similar models such as the OWSM model <cite class="ltx_cite ltx_citemacro_cite">Peng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib24" title="">2024</a>)</cite>, we have only demonstrated our results using the Whisper model.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p">Training of machine learning models is a costly, energy-intensive process, so our method, which introduces a novel means of efficiently adapting existing large pre-trained models to new tasks, may mitigate the ethical concerns about the costs, financial, environmental, or other, associated with training ML models. Furthermore, the success of our approach, specifically cross-task fine-tuning, implies that speech translation systems can be more easily trained for new domains, including languages with limited training resources.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski et al. (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2006.11477" title="">wav2vec 2.0: A framework for self-supervised learning of speech representations</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2006.11477.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bapna et al. (2022)</span>
<span class="ltx_bibblock">
Ankur Bapna, Colin Cherry, Yu Zhang, Ye Jia, Melvin Johnson, Yong Cheng, Simran Khanuja, Jason Riesa, and Alexis Conneau. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2202.01374" title="">mslam: Massively multilingual joint pre-training for speech and text</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2202.01374.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Canavan and Zipperlen (1996)</span>
<span class="ltx_bibblock">
Alexandra Canavan and George Zipperlen. 1996.

</span>
<span class="ltx_bibblock">CALLHOME Mandarin Chinese Speech LDC96S34.

</span>
<span class="ltx_bibblock">Web Download.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu, and Furu Wei. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/jstsp.2022.3188113" title="">Wavlm: Large-scale self-supervised pre-training for full stack speech processing</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</em>, 16(6):1505–1518.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2016)</span>
<span class="ltx_bibblock">
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1604.06174" title="">Training deep nets with sublinear memory cost</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:1604.06174.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al. (2024)</span>
<span class="ltx_bibblock">
Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, Chang Zhou, and Jingren Zhou. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2407.10759" title="">Qwen2-audio technical report</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2407.10759.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. (2021)</span>
<span class="ltx_bibblock">
Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2108.06209" title="">W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2108.06209.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Communication et al. (2023)</span>
<span class="ltx_bibblock">
Seamless Communication, Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, Christopher Klaiber, Pengwei Li, Daniel Licht, Jean Maillard, Alice Rakotoarison, Kaushik Ram Sadagopan, Guillaume Wenzek, Ethan Ye, Bapi Akula, Peng-Jen Chen, Naji El Hachem, Brian Ellis, Gabriel Mejia Gonzalez, Justin Haaheim, Prangthip Hansanti, Russ Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Daniel Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, Mohamed Ramadan, Abinesh Ramakrishnan, Anna Sun, Kevin Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bokai Yu, Pierre Andrews, Can Balioglu, Marta R. Costa-jussà, Onur Celebi, Maha Elbayad, Cynthia Gao, Francisco Guzmán, Justine Kao, Ann Lee, Alexandre Mourachko, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff
Wang, and Skyler Wang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2308.11596" title="">Seamlessm4t: Massively multilingual &amp; multimodal machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2308.11596.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fung et al. (2005)</span>
<span class="ltx_bibblock">
Pascale Fung, Shudong Huang, and David Graff. 2005.

</span>
<span class="ltx_bibblock">HKUST Mandarin Telephone Speech, Part 1 LDC2005S15.

</span>
<span class="ltx_bibblock">Web Download.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2024)</span>
<span class="ltx_bibblock">
Pengcheng Guo, Xuankai Chang, Hang Lv, Shinji Watanabe, and Lei Xie. 2024.

</span>
<span class="ltx_bibblock">Sq-whisper: Speaker-querying based whisper model for target-speaker asr.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsu et al. (2021)</span>
<span class="ltx_bibblock">
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2106.07447" title="">Hubert: Self-supervised speech representation learning by masked prediction of hidden units</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2106.07447.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2021)</span>
<span class="ltx_bibblock">
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2106.09685" title="">Lora: Low-rank adaptation of large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2106.09685.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Inaguma et al. (2019)</span>
<span class="ltx_bibblock">
Hirofumi Inaguma, Kevin Duh, Tatsuya Kawahara, and Shinji Watanabe. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ASRU46091.2019.9003832" title="">Multilingual end-to-end speech translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>, pages 570–577.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ko et al. (2015)</span>
<span class="ltx_bibblock">
Tom Ko, Vijayaditya Peddinti, Daniel Povey, and Sanjeev Khudanpur. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.21437/Interspeech.2015-711" title="">Audio augmentation for speech recognition</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proc. Interspeech 2015</em>, pages 3586–3589.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn (2005)</span>
<span class="ltx_bibblock">
Philipp Koehn. 2005.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2005.mtsummit-papers.11/" title="">Europarl: A parallel corpus for statistical machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of Machine Translation Summit X: Papers</em>, pages 79–86, Phuket, Thailand.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koehn et al. (2007)</span>
<span class="ltx_bibblock">
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondřej Bojar, Alexandra Constantin, and Evan Herbst. 2007.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/P07-2045/" title="">Moses: Open source toolkit for statistical machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions</em>, pages 177–180, Prague, Czech Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le et al. (2024)</span>
<span class="ltx_bibblock">
Chenyang Le, Yao Qian, Long Zhou, Shujie Liu, Yanmin Qian, Michael Zeng, and Xuedong Huang. 2024.

</span>
<span class="ltx_bibblock">Comsl: A composite speech-language model for end-to-end speech-to-text translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Niehues (2024)</span>
<span class="ltx_bibblock">
Danni Liu and Jan Niehues. 2024.

</span>
<span class="ltx_bibblock">Recent highlights in multilingual and multimodal speech translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024)</em>, pages 235–253.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2024a)</span>
<span class="ltx_bibblock">
Hao Ma, Zhiyuan Peng, Mingjie Shao, Jing Li, and Ju Liu. 2024a.

</span>
<span class="ltx_bibblock">Extending whisper with prompt tuning to target-speaker asr.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 12516–12520. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2024b)</span>
<span class="ltx_bibblock">
Rao Ma, Mengjie Qian, Yassir Fathullah, Siyuan Tang, Mark Gales, and Kate Knill. 2024b.

</span>
<span class="ltx_bibblock">Cross-lingual transfer learning for speech translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2407.01130</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ney (1999)</span>
<span class="ltx_bibblock">
H. Ney. 1999.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICASSP.1999.758176" title="">Speech translation: coupling of recognition and translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258)</em>, volume 1, pages 517–520 vol.1.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2019)</span>
<span class="ltx_bibblock">
Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.21437/interspeech.2019-2680" title="">Specaugment: A simple data augmentation method for automatic speech recognition</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Interspeech 2019</em>. ISCA.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2023)</span>
<span class="ltx_bibblock">
Puyuan Peng, Brian Yan, Shinji Watanabe, and David Harwath. 2023.

</span>
<span class="ltx_bibblock">Prompting the hidden talent of web-scale speech models for zero-shot task generalization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.11095</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. (2024)</span>
<span class="ltx_bibblock">
Yifan Peng, Jinchuan Tian, William Chen, Siddhant Arora, Brian Yan, Yui Sudo, Muhammad Shakeel, Kwanghee Choi, Jiatong Shi, Xuankai Chang, et al. 2024.

</span>
<span class="ltx_bibblock">Owsm v3. 1: Better and faster open whisper-style speech models based on e-branchformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2401.16658</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Polok et al. (2024)</span>
<span class="ltx_bibblock">
Alexander Polok, Dominik Klement, Matthew Wiesner, Sanjeev Khudanpur, Jan Černockỳ, and Lukáš Burget. 2024.

</span>
<span class="ltx_bibblock">Target speaker asr with whisper.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2409.09543</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post et al. (2013)</span>
<span class="ltx_bibblock">
Matt Post, Gaurav Kumar, Adam Lopez, Damianos Karakos, Chris Callison-Burch, and Sanjeev Khudanpur. 2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2013.iwslt-papers.14" title="">Improved speech-to-text translation with the fisher and callhome Spanish-English speech translation corpus</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 10th International Workshop on Spoken Language Translation: Papers</em>, Heidelberg, Germany.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2022)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2212.04356" title="">Robust speech recognition via large-scale weak supervision</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2212.04356.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajbhandari et al. (2020)</span>
<span class="ltx_bibblock">
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1910.02054" title="">Zero: Memory optimizations toward training trillion parameter models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:1910.02054.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rouditchenko et al. (2024)</span>
<span class="ltx_bibblock">
Andrew Rouditchenko, Yuan Gong, Samuel Thomas, Leonid Karlinsky, Hilde Kuehne, Rogerio Feris, and James Glass. 2024.

</span>
<span class="ltx_bibblock">Whisper-flamingo: Integrating visual features into whisper for audio-visual speech recognition and translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2406.10082</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salesky et al. (2021)</span>
<span class="ltx_bibblock">
Elizabeth Salesky, Matthew Wiesner, Jacob Bremerman, Roldano Cattoni, Matteo Negri, Marco Turchi, Douglas W. Oard, and Matt Post. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2102.01757" title="">The multilingual tedx corpus for speech recognition and translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2102.01757.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2016)</span>
<span class="ltx_bibblock">
Zhiyi Song, Gary Krug, and Stephanie Strassel. 2016.

</span>
<span class="ltx_bibblock">Gale phase 3 and 4 chinese newswire parallel text.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2021)</span>
<span class="ltx_bibblock">
Jiajia Tang, Kang Li, Xuanyu Jin, Andrzej Cichocki, Qibin Zhao, and Wanzeng Kong. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-long.412" title="">CTFN: Hierarchical learning for multimodal sentiment analysis using coupled-translation fusion network</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 5301–5311, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al. (2022)</span>
<span class="ltx_bibblock">
NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2207.04672" title="">No language left behind: Scaling human-centered machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2207.04672.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2023)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1706.03762" title="">Attention is all you need</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:1706.03762.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020a)</span>
<span class="ltx_bibblock">
Changhan Wang, Juan Pino, Anne Wu, and Jiatao Gu. 2020a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2002.01320" title="">Covost: A diverse multilingual speech-to-text translation corpus</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2002.01320.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020b)</span>
<span class="ltx_bibblock">
Changhan Wang, Anne Wu, and Juan Pino. 2020b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2007.10310" title="">Covost 2 and massively multilingual speech-to-text translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2007.10310.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weiss et al. (2017a)</span>
<span class="ltx_bibblock">
Ron J. Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. 2017a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1703.08581" title="">Sequence-to-sequence models can directly translate foreign speech</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:1703.08581.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weiss et al. (2017b)</span>
<span class="ltx_bibblock">
Ron J. Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. 2017b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.21437/Interspeech.2017-503" title="">Sequence-to-sequence models can directly translate foreign speech</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Interspeech 2017</em>, pages 2625–2629.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wotherspoon et al. (2024)</span>
<span class="ltx_bibblock">
Shannon Wotherspoon, William Hartmann, and Matthew Snover. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2404.11619" title="">Advancing speech translation: A corpus of mandarin-english conversational telephone speech</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2404.11619.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al. (2025)</span>
<span class="ltx_bibblock">
Cihan Xiao, Zejiang Hou, Daniel Garcia-Romero, and Kyu J Han. 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ICASSP49660.2025.10890057" title="">Contextual asr with retrieval augmented large language model</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, pages 1–5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Patel (2024)</span>
<span class="ltx_bibblock">
Ke Zhang and Vishal M. Patel. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2406.13237" title="">Modelmix: A new model-mixup strategy to minimize vicinal risk across tasks for few-scribble based cardiac segmentation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Preprint</em>, arXiv:2406.13237.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Yuhao Zhang, Chen Xu, Bei Li, Hao Chen, Tong Xiao, Chunliang Zhang, and Jingbo Zhu. 2023.

</span>
<span class="ltx_bibblock">Rethinking and improving multi-task learning for end-to-end speech translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2311.03810</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zuluaga-Gomez et al. (2023)</span>
<span class="ltx_bibblock">
Juan Pablo Zuluaga-Gomez, Zhaocheng Huang, Xing Niu, Rohit Paturi, Sundararajan Srinivasan, Prashant Mathur, Brian Thompson, and Marcello Federico. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-main.449" title="">End-to-end single-channel speaker-turn aware conversational speech translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 7255–7274, Singapore. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Training Detail</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Parameter Efficient Fine-tuning</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p">To efficiently adapt the model to these conversational scenarios without overfitting or incurring excessive computational cost, we leverage several parameter-efficient fine-tuning (PEFT) techniques.</p>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<p class="ltx_p">In order to fit the base model into our hardware, we adopt a list of strategies:</p>
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Low-Rank Adaptation (LoRA).</span> LoRA <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib12" title="">2021</a>)</cite> introduces a trainable adapter comprised of rank decomposition matrices on top of the fixed pre-trained model’s weight matrices in specified layers so that the number of trainable parameters can be considerably reduced.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Gradient checkpointing.</span> Gradient checkpointing <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib5" title="">2016</a>)</cite> stores intermediate activations in the forward pass, and re-computes the remaining activations during back-propagation.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Zero Redundancy Optimizer (ZeRO).</span> ZeRO <cite class="ltx_cite ltx_citemacro_cite">Rajbhandari et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib28" title="">2020</a>)</cite> is an algorithm that partitions data, optimizer states, gradients, and parameters for speeding up the training of large neural models with low communication costs.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Hyperparameter Settings</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p">Table <a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A1.T3" title="Table 3 ‣ A.2 Hyperparameter Settings ‣ Appendix A Training Detail ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_tag">3</span></a> presents the hyperparameter configurations used for training our Whisper-UT model.</p>
</div>
<figure class="ltx_table" id="A1.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr class="ltx_tr">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Hyperparameter</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span class="ltx_text ltx_font_bold">Value</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">LoRA Rank</td>
<td class="ltx_td ltx_align_center ltx_border_t">200</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">LoRA Alpha</td>
<td class="ltx_td ltx_align_center">400</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">LoRA Dropout</td>
<td class="ltx_td ltx_align_center">0.1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">Max Training Steps</td>
<td class="ltx_td ltx_align_center ltx_border_t">10000</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Batch Size</td>
<td class="ltx_td ltx_align_center">64</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Gradient Accumulation Steps</td>
<td class="ltx_td ltx_align_center">1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Warmup Steps</td>
<td class="ltx_td ltx_align_center">500</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Learning Rate</td>
<td class="ltx_td ltx_align_center"><math alttext="1\text{e}^{-5}" class="ltx_Math" display="inline" id="A1.T3.m1" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><msup><mtext>e</mtext><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\text{e}^{-5}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left">Weight Decay</td>
<td class="ltx_td ltx_align_center"><math alttext="5\text{e}^{-4}" class="ltx_Math" display="inline" id="A1.T3.m2" intent=":literal"><semantics><mrow><mn>5</mn><mo lspace="0em" rspace="0em">​</mo><msup><mtext>e</mtext><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5\text{e}^{-4}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_t">SpecAug Mask Feature Probability</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.1</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb">SpecAug Mask Time Probability</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.05</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Hyperparameter configurations used for training.</figcaption>
</figure>
<div class="ltx_para" id="A1.SS2.p2">
<p class="ltx_p">Experiments in this work are conducted with 8 V100-32GB GPUs. However, PEFT methods outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A1.SS1" title="A.1 Parameter Efficient Fine-tuning ‣ Appendix A Training Detail ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_tag">A.1</span></a> render the use of 8 GPUs redundant, yet they are deployed to accelerate the training process.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Data Augmentation</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p">We apply the conventional speed perturbation <cite class="ltx_cite ltx_citemacro_cite">Ko et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib14" title="">2015</a>)</cite> with parameters 0.9, 1.0, 1.1 to the speech prior to the training stage. Additionally, we adopt SpecAug <cite class="ltx_cite ltx_citemacro_cite">Park et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib22" title="">2019</a>)</cite> to randomly mask extracted speech features during training.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>CTS Data Detail</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Pre-processing</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p">CTS corpora usually consist of short utterances segmented from a full recording, reflecting the alternating speech of participants during conversations. However, we found empirically that fine-tuning on such segments, presumably due to a mismatch in sample lengths compared to Whisper’s pre-training data, leads to significant performance degradation. The resulting model tends to repetitively produce frequent filler words in the training corpus at inference time regardless of the input. Therefore, we re-segmented the utterances by merging them chronologically, with durations (in seconds) sampled from a Gaussian distribution, e.g. <math alttext="\mathcal{N}(15,5^{2})" class="ltx_Math" display="inline" id="A2.SS1.p1.m1" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>15</mn><mo>,</mo><msup><mn>5</mn><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{N}(15,5^{2})</annotation></semantics></math>. As Whisper’s feature extractor automatically pads the features up to 30 seconds, such re-segmentation also significantly reduced the training cost in terms of memory and time.</p>
</div>
<figure class="ltx_table" id="A2.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Code-switching example with system outputs.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_tt"><span class="ltx_text ltx_font_bold">REF-ASR:</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt">电脑的 MASTER 应该是很 POPULAR 就对了很应该很</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold">HYP-ASR:</span></td>
<td class="ltx_td ltx_align_left">电脑的 master 应该是很 popular 就对了很应该很</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold">REF-MT:</span></td>
<td class="ltx_td ltx_align_left">MASTER degree of computer science it should be very POPULAR it should be</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold">HYP-E2E-ST:</span></td>
<td class="ltx_td ltx_align_left">The computer should be very popular, should be very</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left"><span class="ltx_text ltx_font_bold">HYP-2-Stage-ST:</span></td>
<td class="ltx_td ltx_align_left">The computer’s master should be very popular that’s right very should be very</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb"><span class="ltx_text ltx_font_bold">HYP-MMT:</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb">The computer’s MASTER should be very popular that’s right very should be very</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>BBN-Mandarin Data Specification</h3>
<div class="ltx_para ltx_noindent" id="A2.SS2.p1">
<p class="ltx_p">The BBN Mandarin-English conversational telephony speech (CTS) corpus used in our experiments comprises two primary components:</p>
<ul class="ltx_itemize" id="A2.I1">
<li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">HKUST Mandarin ASR Dataset</span> (90.1 hours): Mandarin conversational speech from telephony interactions, originally designed for ASR research <cite class="ltx_cite ltx_citemacro_cite">Fung et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib9" title="">2005</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">CallHome Mandarin ASR Dataset</span> (20.5 hours): Informal Mandarin dialogues curated for ASR study <cite class="ltx_cite ltx_citemacro_cite">Canavan and Zipperlen (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib3" title="">1996</a>)</cite>.</p>
</div>
</li>
</ul>
<p class="ltx_p">The BBN team <cite class="ltx_cite ltx_citemacro_cite">Wotherspoon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib39" title="">2024</a>)</cite> translated these into English to create parallel speech-to-text translation pairs. While our experiments utilized a pre-publication version provided directly by the BBN authors, minor discrepancies (e.g., data splits, preprocessing, or translation refinements) may exist compared to the final published version. Nevertheless, the corpus retains its core characteristics: conversational telephony domain focus, code-switching prevalence, and disfluency patterns.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Qualitative Analysis of Code-Switching</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p">The code-switching example presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A2.T4" title="Table 4 ‣ B.1 Pre-processing ‣ Appendix B CTS Data Detail ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_tag">4</span></a> demonstrates two critical insights:</p>
</div>
<div class="ltx_para" id="A3.p2">
<ul class="ltx_itemize" id="A3.I1">
<li class="ltx_item" id="A3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i1.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">ASR Preservation of Linguistic Salience:</span> The 2-Stage decoding system successfully retains the code-switched terms “master” and “popular” (WER <math alttext="\approx" class="ltx_Math" display="inline" id="A3.I1.i1.p1.m1" intent=":literal"><semantics><mo>≈</mo><annotation encoding="application/x-tex">\approx</annotation></semantics></math> 0% for these tokens), while E2E-ST completely omits “master”. This suggests that: 1) direct audio-to-translation mapping struggles with lexical disambiguation of homophones (“master” vs. contextually expected “computer”), and 2) explicit intermediate ASR provides discrete textual anchors that guide translation decisions.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i2.p1">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Cross-Modal Faithfulness:</span> While the reference MT (REF-MT) omits the final “很” (translated as "very") from the source utterance “很应该很”, our ASR transcript preserves all repetitions. This discrepancy highlights how audio-derived prosodic cues (e.g., emphatic stress on the final “很”) enable 2Stage-ST and MMT to retain pragmatic emphasis (“…that’s right very should be very”) where text-only MT truncates for conciseness. By aligning acoustic signals (stress patterns) with textual redundancy, our framework distinguishes intentional repetition—a discourse marker of conviction in Mandarin—from superficial noise, demonstrating superior faithfulness to both linguistic content and pragmatic intent compared to E2E ST pipelines.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A3.p3">
<p class="ltx_p">The example validates our hypothesis that two-stage processing particularly benefits scenarios where: 1) ASR can reliably capture linguistically salient content (code-switches, proper nouns), and 2) Audio signals contain complementary paralinguistic information (prosodic boundaries, emphasis) that each modality alone cannot convey. This dual-modality advantage explains 2-Stage-ST’s performance gain over E2E-ST on BBN-Mandarin despite identical model parameters.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Ablation Study</h2>
<figure class="ltx_table" id="A4.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>
Ablation studies on the CTS test sets. The <span class="ltx_text ltx_font_bold">Objective</span> column specifies under which training objective the model system is fine-tuned. The <span class="ltx_text ltx_font_italic">UT</span> objective refers to the unified-translation objective described in section <a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S3.SS5" title="3.5 Unified Training Framework ‣ 3 Methodology ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_tag">3.5</span></a>. The <span class="ltx_text ltx_font_bold">Task</span> column specifies the target inference task. <span class="ltx_text ltx_font_italic">E2E-ST</span> refers to the promptless E2E speech translation setting, <span class="ltx_text ltx_font_italic">MMT</span> refers to the translation process that conditions on both the ground-truth transcript and the speech signals, while <span class="ltx_text ltx_font_italic">2-Stage-ST</span> refers to the MMT process which conditions on the model’s own ASR hypotheses.
</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Objective</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" style="padding-left:5.0pt;padding-right:5.0pt;">
<span class="ltx_text ltx_font_bold">Task</span> (num_beams = 1)</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">ASR</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">E2E-ST</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">MT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">MMT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2-Stage-ST</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">(WER<math alttext="\downarrow" class="ltx_Math" display="inline" id="A4.T5.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">(BLEU<math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T5.m2" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">(BLEU<math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T5.m3" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">(BLEU<math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T5.m4" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">(BLEU<math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T5.m5" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>)</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="11" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">Fisher</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2" style="padding-left:5.0pt;padding-right:5.0pt;">NLLB-1.3B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">None</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="48.3" class="ltx_Math" display="inline" id="A4.T5.m6" intent=":literal"><semantics><mn>48.3</mn><annotation encoding="application/x-tex">48.3</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">2</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{67.3}" class="ltx_Math" display="inline" id="A4.T5.m7" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">67.3</mn><annotation encoding="application/x-tex">\mathbf{67.3}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="8" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">None</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="26.7" class="ltx_Math" display="inline" id="A4.T5.m8" intent=":literal"><semantics><mn>26.7</mn><annotation encoding="application/x-tex">26.7</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="51.6" class="ltx_Math" display="inline" id="A4.T5.m9" intent=":literal"><semantics><mn>51.6</mn><annotation encoding="application/x-tex">51.6</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">4</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ASR</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="19.1" class="ltx_Math" display="inline" id="A4.T5.m10" intent=":literal"><semantics><mn>19.1</mn><annotation encoding="application/x-tex">19.1</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="54.9" class="ltx_Math" display="inline" id="A4.T5.m11" intent=":literal"><semantics><mn>54.9</mn><annotation encoding="application/x-tex">54.9</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">5</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ST</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="20.3" class="ltx_Math" display="inline" id="A4.T5.m12" intent=":literal"><semantics><mn>20.3</mn><annotation encoding="application/x-tex">20.3</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="61.2" class="ltx_Math" display="inline" id="A4.T5.m13" intent=":literal"><semantics><mn>61.2</mn><annotation encoding="application/x-tex">61.2</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">6</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ASR + ST</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="16.3" class="ltx_Math" display="inline" id="A4.T5.m14" intent=":literal"><semantics><mn>16.3</mn><annotation encoding="application/x-tex">16.3</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{62.2}" class="ltx_Math" display="inline" id="A4.T5.m15" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">62.2</mn><annotation encoding="application/x-tex">\mathbf{62.2}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">7</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="60.3" class="ltx_Math" display="inline" id="A4.T5.m16" intent=":literal"><semantics><mn>60.3</mn><annotation encoding="application/x-tex">60.3</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="51.0" class="ltx_Math" display="inline" id="A4.T5.m17" intent=":literal"><semantics><mn>51.0</mn><annotation encoding="application/x-tex">51.0</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="63.4" class="ltx_Math" display="inline" id="A4.T5.m18" intent=":literal"><semantics><mn>63.4</mn><annotation encoding="application/x-tex">63.4</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="61.1" class="ltx_Math" display="inline" id="A4.T5.m19" intent=":literal"><semantics><mn>61.1</mn><annotation encoding="application/x-tex">61.1</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="52.4" class="ltx_Math" display="inline" id="A4.T5.m20" intent=":literal"><semantics><mn>52.4</mn><annotation encoding="application/x-tex">52.4</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">8</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ASR + ST + MT + LM</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{16.0}" class="ltx_Math" display="inline" id="A4.T5.m21" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">16.0</mn><annotation encoding="application/x-tex">\mathbf{16.0}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="61.7" class="ltx_Math" display="inline" id="A4.T5.m22" intent=":literal"><semantics><mn>61.7</mn><annotation encoding="application/x-tex">61.7</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="55.2" class="ltx_Math" display="inline" id="A4.T5.m23" intent=":literal"><semantics><mn>55.2</mn><annotation encoding="application/x-tex">55.2</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">9</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MMT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="16.4" class="ltx_Math" display="inline" id="A4.T5.m24" intent=":literal"><semantics><mn>16.4</mn><annotation encoding="application/x-tex">16.4</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="57.4" class="ltx_Math" display="inline" id="A4.T5.m25" intent=":literal"><semantics><mn>57.4</mn><annotation encoding="application/x-tex">57.4</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="1.4" class="ltx_Math" display="inline" id="A4.T5.m26" intent=":literal"><semantics><mn>1.4</mn><annotation encoding="application/x-tex">1.4</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="67.5" class="ltx_Math" display="inline" id="A4.T5.m27" intent=":literal"><semantics><mn>67.5</mn><annotation encoding="application/x-tex">67.5</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="58.6" class="ltx_Math" display="inline" id="A4.T5.m28" intent=":literal"><semantics><mn>58.6</mn><annotation encoding="application/x-tex">58.6</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">10</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">UT-OOD</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{16.0}" class="ltx_Math" display="inline" id="A4.T5.m29" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">16.0</mn><annotation encoding="application/x-tex">\mathbf{16.0}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="{61.5}" class="ltx_Math" display="inline" id="A4.T5.m30" intent=":literal"><semantics><mn>61.5</mn><annotation encoding="application/x-tex">{61.5}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="44.2" class="ltx_Math" display="inline" id="A4.T5.m31" intent=":literal"><semantics><mn>44.2</mn><annotation encoding="application/x-tex">44.2</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="70.0" class="ltx_Math" display="inline" id="A4.T5.m32" intent=":literal"><semantics><mn>70.0</mn><annotation encoding="application/x-tex">70.0</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="61.6" class="ltx_Math" display="inline" id="A4.T5.m33" intent=":literal"><semantics><mn>61.6</mn><annotation encoding="application/x-tex">61.6</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">11</td>
<td class="ltx_td ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">UT-CTS</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="16.3" class="ltx_Math" display="inline" id="A4.T5.m34" intent=":literal"><semantics><mn>16.3</mn><annotation encoding="application/x-tex">16.3</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="62.0" class="ltx_Math" display="inline" id="A4.T5.m35" intent=":literal"><semantics><mn>62.0</mn><annotation encoding="application/x-tex">62.0</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="55.9" class="ltx_Math" display="inline" id="A4.T5.m36" intent=":literal"><semantics><mn>55.9</mn><annotation encoding="application/x-tex">55.9</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{70.4}" class="ltx_Math" display="inline" id="A4.T5.m37" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">70.4</mn><annotation encoding="application/x-tex">\mathbf{70.4}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{62.1}" class="ltx_Math" display="inline" id="A4.T5.m38" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">62.1</mn><annotation encoding="application/x-tex">\mathbf{62.1}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">12</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" rowspan="11" style="padding-left:5.0pt;padding-right:5.0pt;"><span class="ltx_text ltx_font_bold">BBN</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2" style="padding-left:5.0pt;padding-right:5.0pt;">NLLB-1.3B</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">None</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="8.7" class="ltx_Math" display="inline" id="A4.T5.m39" intent=":literal"><semantics><mn>8.7</mn><annotation encoding="application/x-tex">8.7</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">13</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{22.7}" class="ltx_Math" display="inline" id="A4.T5.m40" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">22.7</mn><annotation encoding="application/x-tex">\mathbf{22.7}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">14</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="8" style="padding-left:5.0pt;padding-right:5.0pt;">Whisper</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">None</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="32.2" class="ltx_Math" display="inline" id="A4.T5.m41" intent=":literal"><semantics><mn>32.2</mn><annotation encoding="application/x-tex">32.2</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="13.0" class="ltx_Math" display="inline" id="A4.T5.m42" intent=":literal"><semantics><mn>13.0</mn><annotation encoding="application/x-tex">13.0</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">15</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ASR</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="18.9" class="ltx_Math" display="inline" id="A4.T5.m43" intent=":literal"><semantics><mn>18.9</mn><annotation encoding="application/x-tex">18.9</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="16.2" class="ltx_Math" display="inline" id="A4.T5.m44" intent=":literal"><semantics><mn>16.2</mn><annotation encoding="application/x-tex">16.2</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">16</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ST</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="23.1" class="ltx_Math" display="inline" id="A4.T5.m45" intent=":literal"><semantics><mn>23.1</mn><annotation encoding="application/x-tex">23.1</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="16.8" class="ltx_Math" display="inline" id="A4.T5.m46" intent=":literal"><semantics><mn>16.8</mn><annotation encoding="application/x-tex">16.8</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">17</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ASR + ST</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="18.5" class="ltx_Math" display="inline" id="A4.T5.m47" intent=":literal"><semantics><mn>18.5</mn><annotation encoding="application/x-tex">18.5</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="20.2" class="ltx_Math" display="inline" id="A4.T5.m48" intent=":literal"><semantics><mn>20.2</mn><annotation encoding="application/x-tex">20.2</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">18</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="37.7" class="ltx_Math" display="inline" id="A4.T5.m49" intent=":literal"><semantics><mn>37.7</mn><annotation encoding="application/x-tex">37.7</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="12.7" class="ltx_Math" display="inline" id="A4.T5.m50" intent=":literal"><semantics><mn>12.7</mn><annotation encoding="application/x-tex">12.7</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="16.0" class="ltx_Math" display="inline" id="A4.T5.m51" intent=":literal"><semantics><mn>16.0</mn><annotation encoding="application/x-tex">16.0</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="20.4" class="ltx_Math" display="inline" id="A4.T5.m52" intent=":literal"><semantics><mn>20.4</mn><annotation encoding="application/x-tex">20.4</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="15.5" class="ltx_Math" display="inline" id="A4.T5.m53" intent=":literal"><semantics><mn>15.5</mn><annotation encoding="application/x-tex">15.5</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">19</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">ASR + ST + MT + LM</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="17.7" class="ltx_Math" display="inline" id="A4.T5.m54" intent=":literal"><semantics><mn>17.7</mn><annotation encoding="application/x-tex">17.7</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="20.4" class="ltx_Math" display="inline" id="A4.T5.m55" intent=":literal"><semantics><mn>20.4</mn><annotation encoding="application/x-tex">20.4</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="14.8" class="ltx_Math" display="inline" id="A4.T5.m56" intent=":literal"><semantics><mn>14.8</mn><annotation encoding="application/x-tex">14.8</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">20</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">MMT</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="17.5" class="ltx_Math" display="inline" id="A4.T5.m57" intent=":literal"><semantics><mn>17.5</mn><annotation encoding="application/x-tex">17.5</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="19.5" class="ltx_Math" display="inline" id="A4.T5.m58" intent=":literal"><semantics><mn>19.5</mn><annotation encoding="application/x-tex">19.5</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="1.0" class="ltx_Math" display="inline" id="A4.T5.m59" intent=":literal"><semantics><mn>1.0</mn><annotation encoding="application/x-tex">1.0</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="25.2" class="ltx_Math" display="inline" id="A4.T5.m60" intent=":literal"><semantics><mn>25.2</mn><annotation encoding="application/x-tex">25.2</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="20.6" class="ltx_Math" display="inline" id="A4.T5.m61" intent=":literal"><semantics><mn>20.6</mn><annotation encoding="application/x-tex">20.6</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;">21</td>
<td class="ltx_td ltx_align_center ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">UT-OOD</td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="17.5" class="ltx_Math" display="inline" id="A4.T5.m62" intent=":literal"><semantics><mn>17.5</mn><annotation encoding="application/x-tex">17.5</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{20.6}" class="ltx_Math" display="inline" id="A4.T5.m63" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">20.6</mn><annotation encoding="application/x-tex">\mathbf{20.6}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="11.1" class="ltx_Math" display="inline" id="A4.T5.m64" intent=":literal"><semantics><mn>11.1</mn><annotation encoding="application/x-tex">11.1</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="25.3" class="ltx_Math" display="inline" id="A4.T5.m65" intent=":literal"><semantics><mn>25.3</mn><annotation encoding="application/x-tex">25.3</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="21.5" class="ltx_Math" display="inline" id="A4.T5.m66" intent=":literal"><semantics><mn>21.5</mn><annotation encoding="application/x-tex">21.5</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;">22</td>
<td class="ltx_td ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:5.0pt;padding-right:5.0pt;">UT-CTS</td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{17.4}" class="ltx_Math" display="inline" id="A4.T5.m67" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">17.4</mn><annotation encoding="application/x-tex">\mathbf{17.4}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="19.8" class="ltx_Math" display="inline" id="A4.T5.m68" intent=":literal"><semantics><mn>19.8</mn><annotation encoding="application/x-tex">19.8</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="15.7" class="ltx_Math" display="inline" id="A4.T5.m69" intent=":literal"><semantics><mn>15.7</mn><annotation encoding="application/x-tex">15.7</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{26.0}" class="ltx_Math" display="inline" id="A4.T5.m70" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">26.0</mn><annotation encoding="application/x-tex">\mathbf{26.0}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:5.0pt;padding-right:5.0pt;"><math alttext="\mathbf{21.6}" class="ltx_Math" display="inline" id="A4.T5.m71" intent=":literal"><semantics><mn class="ltx_mathvariant_bold" mathvariant="bold">21.6</mn><annotation encoding="application/x-tex">\mathbf{21.6}</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p">We conduct ablation experiments presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A4.T5" title="Table 5 ‣ Appendix D Ablation Study ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_tag">5</span></a> on the two CTS datasets (Fisher-Spanish and BBN-Mandarin), as their domain-specific challenges—disfluencies, code-switching, and spontaneous dialogue—diverged significantly from Whisper’s pretraining data. This allows us to isolate our framework’s adaptability beyond pretraining biases and quantify its efficacy in resource-constrained, real-world scenarios.</p>
</div>
<section class="ltx_subsection" id="A4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Text-only MT Training and Its Effects</h3>
<div class="ltx_para" id="A4.SS1.p1">
<p class="ltx_p">Rows 7 and 17 show the results of the MT-only fine-tuning experiment, demonstrating that the model achieves strong text translation performance even with limited in-domain data—BLEU 63.4 on Fisher-Spanish and 16.0 on BBN-Mandarin. This outperforms the original NLLB-1.3B model, though it remains modestly behind its fine-tuned counterpart. This suggests that Whisper’s decoder inherently possesses some text translation capabilities or at least has sufficiently strong source and target language modeling abilities such that minimal adaptation enables it to perform the MT task. Interestingly, this MT training also gives the system MMT ability, as suggested by the 61.1/20.4 (Fisher/BBN) BLEU score, despite MMT being a novel objective that the model was not explicitly trained on. In fact, on the BBN corpus, the MT-trained model exhibits MMT capabilities that surpass its original training objective, achieving a BLEU score of 20.4 (MMT) compared to 16.0 (MT). This finding reinforces our earlier observation of cross-task synergy.</p>
</div>
</section>
<section class="ltx_subsection" id="A4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>Effectiveness of Multi-task Learning</h3>
<div class="ltx_para" id="A4.SS2.p1">
<p class="ltx_p">In rows 6 and 17, we conduct straightforward multi-task fine-tuning experiments by duplicating the speech dataset with both ASR and ST supervision, concatenating the datasets, and employing random sampling within each batch. These experiments confirm that multi-task training is beneficial, as it enhances BLEU score from 61.2 to 62.2 and WER is reduced from 20.3 to 16.3 on the Fisher-Spanish corpus. A similar trend is observed on the BBN set as well. This suggests that jointly optimizing multiple relevant objectives allows the model to better capture linguistic patterns and improve generalization across tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="A4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.3 </span>MMT-Multi-task Training and Its Implications</h3>
<div class="ltx_para" id="A4.SS3.p1">
<p class="ltx_p">Rows 9 and 20 evaluate MMT-multi-task fine-tuned models, that is, the model is trained with <math alttext="q=0" class="ltx_Math" display="inline" id="A4.SS3.p1.m1" intent=":literal"><semantics><mrow><mi>q</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">q=0</annotation></semantics></math> and <math alttext="b=0" class="ltx_Math" display="inline" id="A4.SS3.p1.m2" intent=":literal"><semantics><mrow><mi>b</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">b=0</annotation></semantics></math>. Notably, the MMT inference results outperform even the strong fine-tuned NLLB-1.3B baseline in MT performance, 70.4 vs. 67.4 on Fisher and 26.0 vs. 22.7 on BBN— demonstrating that MMT provides tangible benefits over traditional cascaded MT approaches.</p>
</div>
<div class="ltx_para" id="A4.SS3.p2">
<p class="ltx_p">However, a gap remains between different MMT settings. Specifically, when using the ASR hypothesis as input instead of the ground-truth transcript, i.e., the 2-Stage-ST decoding, performance drops from 67.5 to 58.6 on Fisher and from 25.2 to 20.6 on BBN. While this still exceeds the results from direct ST (52.4 vs. 51.0 on Fisher and 20.6 vs. 19.5 on BBN), the model tends to over-rely on the transcript in the absence of explicit modeling. Specifically, without the special tag to signal potential errors, the model treats the input transcript as fully reliable ground truth—an assumption that breaks down when using ASR outputs, which may contain recognition errors. These highlight both the effectiveness of explicit modeling and the limitations introduced by ASR errors.</p>
</div>
</section>
<section class="ltx_subsection" id="A4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.4 </span>Unified Translation (UT) Training</h3>
<section class="ltx_subsubsection" id="A4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">D.4.1 </span>Overview</h4>
<div class="ltx_para" id="A4.SS4.SSS1.p1">
<p class="ltx_p">Finally, the UT-trained system (rows 11 and 22) achieves the best MMT and 2-Stage-ST results, with MMT reaching 70.4/26.0 BLEU and 62.1/21.6 BLEU, respectively, on the Fisher-Spanish and BBN-Mandarin corpora, proving the method’s effectiveness. Applying the error simulation strategy in this training scheme improves the robustness of the two-stage approach, narrowing the performance gap between MMT and 2-Stage-ST decoding. Specifically, on Fisher, the gap decreases from 8.9 to 8.3 BLEU (row 9 vs. 11), and on BBN, from 4.6 to 4.4 BLEU (row 20 vs. 22), indicating more stable performance under ASR-transcript input.</p>
</div>
</section>
<section class="ltx_subsubsection" id="A4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">D.4.2 </span>Analysis of Transcript-Conditioning</h4>
<div class="ltx_para" id="A4.SS4.SSS2.p1">
<p class="ltx_p">On the Fisher test set, the 2‑Stage‑ST decoding strategy of the Whisper‑UT model actually falls slightly behind the simpler ASR+ST multi‑task E2E‑ST model. Direct multi‑task training of ASR and ST (row 6) achieves a BLEU of 62.2, whereas conditioning on ASR hypotheses under the unified‑translation objective (row 11, 2‑Stage‑ST) yields 62.1—a 0.1 BLEU drop. Through manual inspection, we found this gap is driven largely by inconsistent translation of filler words: the same Spanish filler (e.g., “eh,” “um”) in ASR transcripts is rendered inconsistently in output, magnifying ASR transcript “errors” during translation. Moreover, because Whisper’s ASR and ST performance on Fisher Spanish are both strong already (WER <math alttext="\approx" class="ltx_Math" display="inline" id="A4.SS4.SSS2.p1.m1" intent=":literal"><semantics><mo>≈</mo><annotation encoding="application/x-tex">\approx</annotation></semantics></math> 16, BLEU <math alttext="\approx" class="ltx_Math" display="inline" id="A4.SS4.SSS2.p1.m2" intent=":literal"><semantics><mo>≈</mo><annotation encoding="application/x-tex">\approx</annotation></semantics></math> 60), there is little mismatch for transcript conditioning to resolve, so the transcript signal offers marginal benefit.</p>
</div>
<div class="ltx_para" id="A4.SS4.SSS2.p2">
<p class="ltx_p">In contrast, on the BBN corpus, the UT model demonstrates a clear advantage. The ASR+ST multi‑task E2E‑ST model (row 16) scores 20.2 BLEU, while the Whisper‑UT 2‑Stage‑ST decoder (row 22) jumps to 21.6 BLEU—a significant 1.4‑point gain. This larger benefit arises because BBN combines relatively low WER (<math alttext="\approx" class="ltx_Math" display="inline" id="A4.SS4.SSS2.p2.m1" intent=":literal"><semantics><mo>≈</mo><annotation encoding="application/x-tex">\approx</annotation></semantics></math> 18) with much lower translation quality (BLEU <math alttext="\approx" class="ltx_Math" display="inline" id="A4.SS4.SSS2.p2.m2" intent=":literal"><semantics><mo>≈</mo><annotation encoding="application/x-tex">\approx</annotation></semantics></math> 20), indicating that the model’s ST ability lags behind its ASR competence. In this scenario, explicitly leveraging ASR transcripts helps fill the performance gap, yielding more accurate translations under the unified objective.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.5 </span>Impact of Out-of-Domain Text Data</h3>
<section class="ltx_subsubsection" id="A4.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">D.5.1 </span>Dataset Setup</h4>
<div class="ltx_para" id="A4.SS5.SSS1.p1">
<p class="ltx_p">To evaluate the robustness of our unified framework to domain shifts in text data, we replace the in-domain machine translation (MT) pairs (derived from CTS audio transcripts, as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#S4.SS3" title="4.3 Training ‣ 4 Experiments ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_tag">4.3</span></a>) with out-of-domain (OOD) text pairs. Specifically:</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS5.SSS1.p2">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Spanish:</span> We use 197 hours of text pairs from three sources:</p>
<ul class="ltx_itemize" id="A4.I1">
<li class="ltx_item" id="A4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i1.p1">
<p class="ltx_p">CoVoST 2 <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib36" title="">2020b</a>)</cite> (diverse web-mined speech),</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i2.p1">
<p class="ltx_p">mTEDx <cite class="ltx_cite ltx_citemacro_cite">Salesky et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib30" title="">2021</a>)</cite> (TED talk subtitles), and</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I1.i3.p1">
<p class="ltx_p">Europarl-ST <cite class="ltx_cite ltx_citemacro_cite">Koehn (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib15" title="">2005</a>)</cite> (parliamentary proceedings).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS5.SSS1.p3">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Mandarin:</span> We include 130 hours from:</p>
<ul class="ltx_itemize" id="A4.I2">
<li class="ltx_item" id="A4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I2.i1.p1">
<p class="ltx_p">CoVoST <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib35" title="">2020a</a>)</cite> (multilingual web content),</p>
</div>
</li>
<li class="ltx_item" id="A4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I2.i2.p1">
<p class="ltx_p">GALE <cite class="ltx_cite ltx_citemacro_cite">Song et al. (<a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#bib.bib31" title="">2016</a>)</cite> (broadcast news and interviews), and</p>
</div>
</li>
<li class="ltx_item" id="A4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A4.I2.i3.p1">
<p class="ltx_p">proprietary in-house datasets (mixed genres).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A4.SS5.SSS1.p4">
<p class="ltx_p">The OOD sets contrast sharply with CTS data in domain (e.g., formal talks vs. casual dialogues) and lexical style. To isolate the effect of data domain (not scale), we match the total training steps to our baseline CTS experiments, ensuring comparable optimization cycles. This setup tests whether cross-modal alignment generalizes to heterogeneous text distributions.</p>
</div>
<figure class="ltx_table" id="A4.T6">
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt"><span class="ltx_text ltx_font_bold">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="4"><span class="ltx_text ltx_font_bold">Pre-training</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span class="ltx_text ltx_font_bold">Fine-tuning (hrs)</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="3"><span class="ltx_text ltx_font_bold">Speech (hrs)</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">Text</span></td>
<td class="ltx_td"></td>
<td class="ltx_td"></td>
<td class="ltx_td"></td>
<td class="ltx_td"></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_border_r"></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">ASR</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">ST</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">Total</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">(token | sentence)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">3-Way</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">ASR-only</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">ST-only</span></td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">Total</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Whisper-large-v2</td>
<td class="ltx_td ltx_align_center ltx_border_t">
<math alttext="555" class="ltx_Math" display="inline" id="A4.T6.m1" intent=":literal"><semantics><mn>555</mn><annotation encoding="application/x-tex">555</annotation></semantics></math>k</td>
<td class="ltx_td ltx_align_center ltx_border_t">
<math alttext="126" class="ltx_Math" display="inline" id="A4.T6.m2" intent=":literal"><semantics><mn>126</mn><annotation encoding="application/x-tex">126</annotation></semantics></math>k</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math alttext="680" class="ltx_Math" display="inline" id="A4.T6.m3" intent=":literal"><semantics><mn>680</mn><annotation encoding="application/x-tex">680</annotation></semantics></math>k</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">–</td>
<td class="ltx_td ltx_align_center ltx_border_t">–</td>
<td class="ltx_td ltx_align_center ltx_border_t">–</td>
<td class="ltx_td ltx_align_center ltx_border_t">–</td>
<td class="ltx_td ltx_align_center ltx_border_t">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">NLLB-1.3B</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center ltx_border_r">–</td>
<td class="ltx_td ltx_align_center ltx_border_r">N/A | <math alttext="&gt;40" class="ltx_Math" display="inline" id="A4.T6.m4" intent=":literal"><semantics><mrow><mi></mi><mo>&gt;</mo><mn>40</mn></mrow><annotation encoding="application/x-tex">&gt;40</annotation></semantics></math>B</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center">–</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">SeamlessM4T-Large</td>
<td class="ltx_td ltx_align_center">N/A</td>
<td class="ltx_td ltx_align_center">N/A</td>
<td class="ltx_td ltx_align_center ltx_border_r">
<math alttext="&gt;1" class="ltx_Math" display="inline" id="A4.T6.m5" intent=":literal"><semantics><mrow><mi></mi><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">&gt;1</annotation></semantics></math>M</td>
<td class="ltx_td ltx_align_center ltx_border_r">N/A | <math alttext="&gt;40" class="ltx_Math" display="inline" id="A4.T6.m6" intent=":literal"><semantics><mrow><mi></mi><mo>&gt;</mo><mn>40</mn></mrow><annotation encoding="application/x-tex">&gt;40</annotation></semantics></math>B</td>
<td class="ltx_td ltx_align_center">N/A</td>
<td class="ltx_td ltx_align_center">N/A</td>
<td class="ltx_td ltx_align_center">N/A</td>
<td class="ltx_td ltx_align_center">
<math alttext="&gt;400" class="ltx_Math" display="inline" id="A4.T6.m7" intent=":literal"><semantics><mrow><mi></mi><mo>&gt;</mo><mn>400</mn></mrow><annotation encoding="application/x-tex">&gt;400</annotation></semantics></math>k</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">STAC-ST</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center ltx_border_r">–</td>
<td class="ltx_td ltx_align_center ltx_border_r">–</td>
<td class="ltx_td ltx_align_center"><math alttext="206" class="ltx_Math" display="inline" id="A4.T6.m8" intent=":literal"><semantics><mn>206</mn><annotation encoding="application/x-tex">206</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center"><math alttext="206" class="ltx_Math" display="inline" id="A4.T6.m9" intent=":literal"><semantics><mn>206</mn><annotation encoding="application/x-tex">206</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Bi-NMT</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center ltx_border_r">–</td>
<td class="ltx_td ltx_align_center ltx_border_r">–</td>
<td class="ltx_td ltx_align_center"><math alttext="206" class="ltx_Math" display="inline" id="A4.T6.m10" intent=":literal"><semantics><mn>206</mn><annotation encoding="application/x-tex">206</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center"><math alttext="206" class="ltx_Math" display="inline" id="A4.T6.m11" intent=":literal"><semantics><mn>206</mn><annotation encoding="application/x-tex">206</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Multi-ST</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center ltx_border_r">–</td>
<td class="ltx_td ltx_align_center ltx_border_r">–</td>
<td class="ltx_td ltx_align_center"><math alttext="472" class="ltx_Math" display="inline" id="A4.T6.m12" intent=":literal"><semantics><mn>472</mn><annotation encoding="application/x-tex">472</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center"><math alttext="472" class="ltx_Math" display="inline" id="A4.T6.m13" intent=":literal"><semantics><mn>472</mn><annotation encoding="application/x-tex">472</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">Multi-ASR</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center ltx_border_r">–</td>
<td class="ltx_td ltx_align_center ltx_border_r">–</td>
<td class="ltx_td ltx_align_center"><math alttext="269" class="ltx_Math" display="inline" id="A4.T6.m14" intent=":literal"><semantics><mn>269</mn><annotation encoding="application/x-tex">269</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center">–</td>
<td class="ltx_td ltx_align_center"><math alttext="269" class="ltx_Math" display="inline" id="A4.T6.m15" intent=":literal"><semantics><mn>269</mn><annotation encoding="application/x-tex">269</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_r">QWen2-Audio</td>
<td class="ltx_td ltx_align_center">N/A</td>
<td class="ltx_td ltx_align_center">N/A</td>
<td class="ltx_td ltx_align_center ltx_border_r">
<math alttext="&gt;5" class="ltx_Math" display="inline" id="A4.T6.m16" intent=":literal"><semantics><mrow><mi></mi><mo>&gt;</mo><mn>5</mn></mrow><annotation encoding="application/x-tex">&gt;5</annotation></semantics></math>M</td>
<td class="ltx_td ltx_align_center ltx_border_r">
<math alttext="2.4" class="ltx_Math" display="inline" id="A4.T6.m17" intent=":literal"><semantics><mn>2.4</mn><annotation encoding="application/x-tex">2.4</annotation></semantics></math>T | N/A</td>
<td class="ltx_td ltx_align_center">N/A</td>
<td class="ltx_td ltx_align_center">N/A</td>
<td class="ltx_td ltx_align_center">N/A</td>
<td class="ltx_td ltx_align_center">
<math alttext="520" class="ltx_Math" display="inline" id="A4.T6.m18" intent=":literal"><semantics><mn>520</mn><annotation encoding="application/x-tex">520</annotation></semantics></math>k</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r"><span class="ltx_text ltx_font_bold">Whisper-UT (Ours)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb">
<math alttext="555" class="ltx_Math" display="inline" id="A4.T6.m19" intent=":literal"><semantics><mn>555</mn><annotation encoding="application/x-tex">555</annotation></semantics></math>k</td>
<td class="ltx_td ltx_align_center ltx_border_bb">
<math alttext="126" class="ltx_Math" display="inline" id="A4.T6.m20" intent=":literal"><semantics><mn>126</mn><annotation encoding="application/x-tex">126</annotation></semantics></math>k</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">
<math alttext="680" class="ltx_Math" display="inline" id="A4.T6.m21" intent=":literal"><semantics><mn>680</mn><annotation encoding="application/x-tex">680</annotation></semantics></math>k</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">–</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><math alttext="110\sim 180" class="ltx_Math" display="inline" id="A4.T6.m22" intent=":literal"><semantics><mrow><mn>110</mn><mo>∼</mo><mn>180</mn></mrow><annotation encoding="application/x-tex">110\sim 180</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb">–</td>
<td class="ltx_td ltx_align_center ltx_border_bb">–</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><math alttext="110\sim 180" class="ltx_Math" display="inline" id="A4.T6.m23" intent=":literal"><semantics><mrow><mn>110</mn><mo>∼</mo><mn>180</mn></mrow><annotation encoding="application/x-tex">110\sim 180</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Comparison of pre-training and fine-tuning data scales for Whisper-UT and baseline models. “3-Way Parallel” refers to datasets with aligned speech, transcripts, and translations. Note that "N/A" means some data is used for the specific training, yet the exact amount is not available.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="A4.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">D.5.2 </span>Analysis of OOD Text Data Injection</h4>
<div class="ltx_para" id="A4.SS5.SSS2.p1">
<p class="ltx_p">Injecting out‑of‑domain text under the unified objective appears to have limited benefit and in some cases even disrupted established behaviors. On Fisher, UT‑OOD (row 10) lags behind UT‑CTS across every translation metric—most notably MT accuracy, which jumps from 44.2 BLEU with OOD data to 55.9 BLEU when text is drawn from the CTS domain. This suggests that the linguistic and stylistic mismatch of web‑mined, TED talk, and parliamentary text fails to reinforce the speech‑to‑text alignment learned on conversational telephone speech, and may inject conflicting patterns that the model struggles to reconcile.</p>
</div>
<div class="ltx_para" id="A4.SS5.SSS2.p2">
<p class="ltx_p">A similar story unfolds on BBN. On BBN, the impact of injecting OOD text is most pronounced in the MT task. Under UT‑OOD (row 21), the model’s MT performance barely improves over the base unified setting and remains far below the CTS‑matched variant—rising only to 11.1 BLEU compared with 15.7 BLEU for UT‑CTS (row 22). In contrast, UT‑CTS consistently lifts MT and MMT performance by several BLEU points and slightly improves ASR quality. Together, these findings imply that substituting in‑domain transcripts with heterogeneous text corpora does not generalize well in a cross‑modal training regime and can inadvertently weaken the model’s ability to leverage the unified translation objective.</p>
</div>
</section>
</section>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Model Training Data Overview</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p">Here, we also present a rough sketch of the training data amounts for our model and the compared methods, as summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2509.16375v1#A4.T6" title="Table 6 ‣ D.5.1 Dataset Setup ‣ D.5 Impact of Out-of-Domain Text Data ‣ Appendix D Ablation Study ‣ Whisper-UT: A Unified Translation Framework for Speech and Text"><span class="ltx_text ltx_ref_tag">6</span></a>. However, it is important to note that due to differences in training methodologies, stages, and the unavailability of precise details for some systems, this comparison should be interpreted with caution and may contain ambiguities. We encourage readers to consult the original publications for more accurate and comprehensive descriptions of the training data used in each model.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 19 19:33:14 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
